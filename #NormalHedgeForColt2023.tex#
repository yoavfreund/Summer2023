\documentclass{article}[12pt]
\usepackage{fullpage}
% Packages
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{graphicx}

\input{macros}

\title{Second order bounds for NormalHedge}

\author{Yoav Freund}
\begin{document}

\maketitle
\begin{abstract}
  We study potential-based online learning algorithms in a game
  theoretical setting. We characterize the min/max optimal strategies
  for the set of potential functions that have strictly positive
  derivatives up to order four. We show that the optimal adversarial
  strategy for all of these cases corresponds to Brownian Motion.

  Based on this analysis we design a variant of NormalHedge which has
  second order upper bounds that are closely matched by lower-bounds.
\end{abstract}

\section{Introduction}
In this paper we study the {\em the decision-theoretic online learning game}
(DTOL)~\cite{freund1997decision}. DTOL (Figure~\ref{fig:DTOL}) is a
repeated zero sum game between a {\em learner} and an {\em
  adversary}. The adversary controls the losses of $N$ actions, while
the learner controls a distribution over the actions.

\begin{figure}[ht!]
\framebox{
\begin{minipage}[t]{6.4in}
For $t=1,\ldots,T$
\begin{enumerate}
\item The learner chooses a weight function $P_j^t$ over the
  actions $j \in \{1,\ldots,N\}$ \\
      such that $\sum_{j=1}^N P_j^t=1$  
    \item The adversary chooses an {\em instantaneous loss} for each
      of the $N$ actions: \\
      $l_j^t \in [-1,+1]$ for $j \in \{1,\ldots,N\}$.
    \item The {\em cumulative loss of action $j$} 
    is  $L^t_j = \sum_{s=1}^t l_j^s$. 
    \item The learner incurs an {\em instantaneous average loss} defined as
      $\ell^t = \sum_{j=1}^N P^t_j l_j^t$
    \item The {\em cumulative loss of the learner} is
      $L_\ell^t = \sum_{s=1}^t \ell^s$
    \item The {\em cumulative regret} of the learner with respect to
      action $j$ is
      $\R_j^t = L_\ell^t -L_j^t $.
\end{enumerate}
\end{minipage}}
\caption{Decision theoretic online learning \label{fig:DTOL}}
\end{figure}

\cite{freund1997decision} presents the DTOL framework and applies the
weighted majority algorithm~\cite{LittlestoneWa94} to define the {\em
  multiplicative weights algorithm} which yield {\em zero-order}
regret bounds of the form
\begin{equation} \label{eqn:0-order-bound}
  \max_{j=1}^N \R_j^T \prec \sqrt{T \ln N}
\end{equation}
We use $\prec$ to indicate that we ignore lower order terms. 

Many generalizations and refinements have been made over the years. We
describe those that are relevant to our paper below
\begin{itemize}
\item{\bf percentile-based bounds} \cite{freund1999adaptive} refines
  this bound by replacing the comparison to the best single action to
  a comparison to the regret to the top $\epsilon$-percentile of the
  actions, indicated by $\R(T,\epsilon)$, giving bounds of the form
\begin{equation} \label{eqn:percentile-bounds}
\R(T,\epsilon) \prec \sqrt{T \ln \frac{1}{\epsilon}}
\end{equation}
Using quantiles allows actions sets that are uncountably infinite.
For example consider an action set that corresponds to linear
functions with real valued parameters.

\item{\bf Parameter-free algorithms} The multiplicative weights
  algorithm has a learning rate parameter $\eta>0$. Achieving the
  bounds~\ref{eqn:0-order-bound} and~\ref{eqn:percentile-bounds}
  requires a priori-knowledge of $T$ and either $N$ or       
  $\epsilon$. ``Parameter-free'' algorithms remove the need for
  a-priori knowledge of the sequence. Parameter-free algorithms have
  been proposed in~\cite{chaudhuri2009parameter,
    chernov2010prediction,orabona2016coin,cutkosky2018black}.

\item{\bf Second order bounds} take advantage of so-called ``easy
  sequences'' where $|l_j^i|<1$. Roughly speaking, second order bounds
  replace the length of the sequence $T$ with a sum of quadratic terms
  of the form $(l_j^t)^2$ or $(l_j^t-\ell_t)^2$. Two types of second
  order bounds were proposed in~\cite{cesa2007improved}.  The {\em
    Quadratic Variation} $Q_j^T=\sum_{t=1}^T (l_j^t)^2$ measures the
  cumulative variation of the action $j$ at. While the {\em cumulative
    variance}
  $V^T = \sum_{t=1}^T \sum_{j=1}^N P^t_j (l_j^t - \ell^t)^2$ is a
  measure of the variance of all of the actions. There are numerous
  publications that give bounds based on quadratic
  variation~\cite{chernov2009prediction,hazan2010extracting,gaillard2014second,koolen2015second}. But
  none that are based on cumulative variance. In this paper we give an
  upper and lower bounds based on cumulative variance. Thereby
  providing evidence that the cumultive variance provides a tighter
  characterization of ``easy sequences'' than quadratic variation.
  
\item {\bf Potential based Online learning algorithms} There are two
  main paradigms for designing online learning algorithms: the {\em
    follow the leader} paradigm and the {\em potential function}
  paradigm.  Our work here is in the potential function paradigm. A
  potential function is a positive increasing function
  $\pot:\reals \to \reals$. The main quantity that is analyzed is the
  score, the average potential the actions,
  $\score^t = \frac{1}{N} \sum_{j=1}^N \phi(R_j^t)$. The regret buonds
  are proven by combining an upper and lower bound on $\score^t $.
\end{itemize}

\subsection{Main Results}
In this paper we present a new analysis of the algorithm NormalHedge
presented in~\cite{sec:NormalHedgeAlg} and improved upon
in~\cite{luo2015achieving}.  NormalHedge is potential based and
parameter free, the bounds on its performance or of the
form(~\ref{eqn:percentile-bounds}).

\paragraph*{Second order uppper bound on NormalHedge}
Our first contribution is to prove a second order bound on NormalHedge:
\begin{theorem} \label{thm:NHUpperBound}
The algorithm defined in Figure~\ref{fig:normalhedge2} satisfies the following bound:
\begin{equation} \label{eqn:upper-bound}
\forall T,\epsilon:\;\; \R^T_{\epsilon}\leq \sqrt{(V^T+1) \left( \ln (V^T+1) + 2 \ln \frac{1}{\epsilon}\right)}
\mbox{  where  } V^T=\frac{1}{N} \sum_{t=1}^T \sum_{j=1}^N H^t_j (l_j^t-\ell^t)^2
\end{equation}
Where for iteration $t$, $(H_1^t,\ldots, H_N^t)$ is a distribution
over the actions $1,\ldots,N$ (Different from $P_i^j$) defined
in~(\ref{eqn:HNormalHedge})
\end{theorem}

\paragraph*{General Lower bound}
We give an almost-matching second order lower bound depending on $V^T$.

For the sake of comparing with the lower bound we rewrite the upper bound on $R^T_\epsilon$ as an upper
bound on $\epsilon$ as a function of $T$ and $R$:
\begin{equation} \label{eqn:epsilonUpperBound}
  \forall t,R,\;\; \epsilon(t,R) \leq \sqrt{V^t+1} \;\; \exp \paren{\frac{-R^2}{2(V^t+1)}}
\end{equation}

We state our lower bound in a similar form:
\begin{theorem} \label{thm:LowerBound} Let $V^0,V_1,\ldots$ be a
  non-decreeasing sequence of positive real numbers such that $V^0=0$,
  $\forall t \geq 0, V^t\leq V^{t+1} \leq V^t+1$ and
  $\lim_{t \to \infty} V_t = \infty$.  Then there is an adversarial
  strategy such that, regardless of the actions of the learner, $V_t$
  is the cumulative variance for iteration $t$ and for any $\delta>0$
  there exists a large enough $T(\delta)$ such that
  \begin{equation} \label{eqn:epsilonLowerBound}
    \forall t > T(\delta),\;\;\forall R,\;\; \epsilon(t,R) +\delta \geq
    \left(\frac{\sqrt{V^t}}{\R} -  \left( \frac{\sqrt{V^t}}{\R} \right)^3 \right) \exp \paren{\frac{-R^2}{2V^t}}
  \end{equation}
\end{theorem}

% Let $S_t = \sum_{i=1}^t \sigma_i^2$ for $0 \geq \sigma_i \leq 1$ be a sequence
% such that $\lim_{t \to \infty}   S_t =\infty$.

Note that the ratio between the upper and lower bound on
$\epsilon(t,R)$ is, to firstorder, $R$. As the dependence of the
regret bound~\ref{eqn:upper-bound} on $\epsilon$ is a term of the form
$\log 1/\epsilon$ is we get that in the lower bound the term
$2\log \frac{1}{\epsilon}$ is replaced with
$2 \log \frac{1}{\epsilon R}$ which is a logarithmic gap between the
upper and lower bound on $R^t_\epsilon$.

\paragraph{Min max analysis of potential-based algorithms}
The lower bound~\ref{eqn:epsilonLowerBound} holds for any DTOL
algorithm and leaves a small gap with the upper bound
~\ref{eqn:epsilonUpperBound}. However, if we restrict ourselves to
potential-based online algrithms we can characterize the min/max
algorithm and adversary. This analysis holds for a broad set of
potential functions beyond the NormalHedge potential.  We formalize
the notion of potential based learning by defining a variant of the
DTOL game, where the goal of the learner is to minimize the average
potential. We derive the min-max solution of this game for any
potential function that has four strictly positive derivatives. In
particular, we show that the optimal strategy for the adversary
corresponds to Brownian Motion.

\subsection{Relation To Other Work}

The analysis described here builds on a long line of work. Including
the Binomial Weights algorithm and it's
variants~\cite{cesa1996line,abernethy2006continuous,abernethy2008optimal}
as well as drifting games~\cite{schapire2001drifting,freund2002drifting}.

\subsection{Organization of the paper}
The rest of the paper is organized as follows. In
Section~\ref{sec:NormalHedgeAlg}, we describe the algorithm that
satisfies Theorem~\ref{thm:NHUpperBound}. The rest of the paper is
devoted to the analysis of this and related algorithms.

In Section~\ref{sec:preliminaries} We define some terms and
notation. In Section~\ref{sec:PotentialGame} we define the potential
game which captures the structure of online learning algorithms that
have an and has an a-priori known end (or horizon) and a fixed
potential function for the end.  Having a known horizon and known
final potential function is needed for our analysis. Ultimately the
bounded horizon assumption is removed to allow algorithms, such as
NormalHedge, that can be stopped at any iteration.

We formalize the notion of the {\em state} of the game $\state$, which
allows us to separately analyze each iteration. In
Section{sec:potentialRecursion} we show how, for a fixed pair of
learner and adversarial strategies, we can extend the potential at the
horizon to a potential for any earlier iteration.

In Section~\ref{sec:disc-game-strategies} we describe strategies for the adversary and for the learner.

\section{The NormalHedge  Algorithm} \label{sec:NormalHedgeAlg}
Figure~\ref{fig:normalhedge2} describes a variant of the NormalHedge
algorithm~\cite{chaudhuri2009parameter} which satisfies the regret
bound given in Theorem~\ref{eqn:upper-bound}

In fact, the original algorithm can be used. The only difference is in
the way the time (here - the cumulative variance) is updated. While
the original version requires solving a system of two non-linear
equations with two unknowns, the method here is simple averaging.
This simpler update lends itself to the second order analysis which is
obscured in~\cite{chaudhuri2009parameter}.

\newcommand{\pospart}[1]{[#1]_{+}}

\begin{figure}[ht!]
\framebox{
\begin{minipage}[t]{6.4in}
Set $V_0=1$ \\
For $t=0,1,2,\ldots$

\begin{enumerate}
\item The learner chooses a distribution

\begin{equation} \label{eqn:learner-strat-cc}
  \learnerM^{NH2}(t,j) =  \frac{1}{Z^t} \pospart{R_j^t} \exp\paren{\frac{\pospart{R_j^t}^2}{2V_t}}
  \mbox{ where } Z^t = \sum_{j=1}^N \pospart{R_j^t} \exp\paren{\frac{\pospart{R_j^t}^2}{2V_t}}
\end{equation}

\item The aggregate loss is calculated:
  \begin{equation} %\label{eqn:ell-discrete}
    \ell^t=\sum_{j=1}^N \learnerM^{NH2}(t,j) l_j^t,
  \end{equation}
  
\item  Increment $V_{t+1}=V_t +Var_t$ where
\begin{equation} \label{eqn:deltat}
  Var_t=  \sum_{j=1}^N H(V_t+1,\R_j^t+1)(l_j^t-\ell_t)^2
\end{equation}

\begin{equation}  \label{eqn:HNormalHedge}
W(V,\R) = 
  \exp\paren{\frac{\pospart{R}^2}{2V}} \left(\frac{1}{2V^{3/2}} + \frac{\pospart{R}^2}{V^{5/2}} \right),\;
  H(V,\R) = \frac{W(V,R)}{\sum_{j=1}^N W(V,R_j)}
\end{equation}

\item Update regrets $\forall j=1,\ldots N,\;\;\; R_j^{t+1} = R_j^t +l_j^t -\ell^t $

\end{enumerate}
\end{minipage}}
\caption{NormalHedge.2 \label{fig:normalhedge2}}
\end{figure}

\section{Preliminaries} \label{sec:preliminaries}

We define some terms and notations that will be used in the rest of the paper.

{\bf  Generalized binomial distribution}
We denote by $\Binom(n,s)$ the distribution over the reals defined by
$\sum_{i=1}^n X_i$ where $X_i$ are iid binary random variables which
attain the values $-s,+s$ with equal probabilities.

{\bf Strict Positivity} We restrict ourselves to potential functions have
positive derivatives for degrees $0$ to $k$.  To that end we use the
following definition:
\begin{definition}[Strict Positivity of degree $k$]
A function $f:\reals \to \reals$ is strictly positive of degree $k$, 
denoted $f \in \SP{k}$ if the derivatives of orders 0 to $k$:  
$f(x), \frac{d}{dx}f(x), \ldots, \frac{d^k}{dx^k}f(x)$ exist and are strictly positive.
\end{definition}
The following useful lemma states that $\SP{k}$ is closed under positive conbinations.
\begin{lemma}  \label{lemma:SP-pos-comb}
  Suppose that for $i =1,\ldots,n$, $f_i \in \SP{k}$ and $\alpha_i>0$,
  Then $\sum_{i=1}^k \alpha_i f_i \in \SP{k}$
\end{lemma}


\section{The potential game}  \label{sec:PotentialGame}

Our analysis is based on modeling decision theoretic learning as a
full information, deterministic, iterative game.  Central to our model
is the concept of {\em state}. Roughly speaking, the state of the game
is a summary of the history which holds all of the relevant past
information. From Figure~\ref{fig:DTOL} it is clear that the state on
iteration $i$ can be defined as the combination of the cumulative loss
of the algorithm $L_\ell^i$ and the cumulative regret of the $N$
actions: $\left\langle L^i_\ell,R_1^i,\ldots,R_N^i \right\rangle$. As
the indices of the actions can be arbitrarily permuted, the cumulative
losses of the actions can be represented as a {\em bag} which can also
be thought of as a point-mass distribution over $\reals$. Here we
allow arbitrary distribution over the cumulative loss of the
expert.~\footnote{More precisely, an Borel-Meaure on $\reals$.} This
does not substrantially expand the setup with $N$ acctions as any
Borel-measure can approximated by letting $N \to \infty$. We use the
term {\em state} at time $i$, denoted $\state(i)$, to represent
distribution or measure, over regrets at iteration $i$.
The full state at time $i$ is
$\fstate(i) \doteq \left\langle L^i_\ell,\state(i)
\right\rangle$.~\footnote{Note that according to our definitions the
  state is unchanged when exchanging the indices $i \neq j$ when
  $L_i^i=L_j^i$.  On the other hand, algorithms that depend on
  quadrative variation, depend also on a quadratic expression for each
  action, which is problem with second order algorithms on quadratic
  variation, as these depend on information outside
  $\left\langle L^i_\ell,\state(t) \right\rangle$, namely, the
  quadratic variation of each action.{\bf to be completed}}

Using this generalized definition of state we define a variant of the
DTOL game (Figure~\ref{fig:DTOL}) that we call the discrete time game (DTG)
(Figure~\ref{fig:discrete-Time-Game}). DTG generalizes DTOL and strengthens the adversary,
but not the learner. Thus any upper bound on the regret that holds for DTG holds also for DTOL. 

We now explain the elements of DGT in Figure~\ref{fig:discrete-Time-Game}
\begin{itemize}
\item the loss $l_j^i$ associated with action is generalized to a measureble function that maps 
\item At the beginning of iteration $i$ the adversary chooses a
  step-size $1 \geq s_i \geq 0$ which sets the range of instatanous
  losss/gains to $[-s_i,+s_i]$
\item We replace the iteration number $i$ with $U_i$ that is an upper
  bound on the cumulative variance that is updated according to the recursion:
  $U_i = U_{i-1}+s_i^2$.
\item  
\end{itemize}


The DTOL game progresses from the full state $\fstate(t)$ to $\fstate(t+1)$ according to the strategies of the learner and the adversary:
\begin{enumerate}
\item The learner chooses a weighting $P(t,R)$ as a function of $\fstate(t)$
\item The adversary assigns a loss to each action. $\adversM(t,\cdot): \reals \to \Delta^{[-s_i,+s_i]}$
  \item ,,
\end{enumerate}




$\left\langle L^i_\ell,\state(t) \right\rangle$ to
$\left\langle L^{t+1}_\ell,\state(t+1) \right\rangle$


a sequence of
states $s_0,s_1,s_2,\ldots,s_t$, and a strategy for the adversary
(potentially randomized) $A: S \times \Omega \to \vec{l}_t$, a
strategy for the learner $L:S,\Omega \to \vec{P}$.

such as DTOL described in figure~\ref{fig:DTOL}. 

\subsection{State symmetries}


Intuitively, the state of a
game captures all of the information about the game history that is
relevant to the game's future. More formally, a state is a function of
history that such that the future is independent of the history given
the state. One can define the state of the DTOL game at time $t$ as
the vector consisting of the $N$ cumulative regrets:
$(R_1^i,\ldots,R_N^i)$. Note also that the regret (relative to the
best action or a percentile) as well as the potential are conserved
under permutation of the regrets. Therefor, a smaller definition of
the state is the {\em distribution} of the cumulative regrets.

When the set of actions is finite, this distribution is a point mass
distribution where the probability of a single point corresponds to
the number of actions with the cumulative regret.

We allow countable and uncountable sets of actions, however, for most of our analysis it sufficies to consider 
finite action sets that double in size at each step.

Specifically we assume that the adversary chooses a step size $1 \leq s \leq 0$. At time $t$ the adversary
associates $2^t$ actions with the $2^t$ loss sequences of the form $\{\pm s\}^t$.

Thee set of ``divisible'' actions is somewhat larger. Here we allow
the adversary to split each action at each stage into two, each action
can have a different weight, but the weights have to sum that of the
action before it is split.

\begin{figure}[ht!]
\framebox{
\begin{minipage}[t]{6.4in}
Initialization: $U_0=0$ \newline
Length of game (Horizon): $\realT>0$\newline

On iteration $i=1,2,\ldots$
\begin{enumerate}
\item  If $U_{i}= \realT$ the game terminates.
\item The adversary chooses a {\em step size} $0<s_i \leq \min(\sqrt{\realT-U_i},1)$, which advances
  time by $U_i = U_{i-1}+s_i^2$
\item Given $s_i$, the learner chooses a distribution $\learnerM(i)$ over $\reals$.
\item The adversary chooses a mapping from $\reals$ to distributions
  over $[-s_i,+s_i]:\;\adversM(t,\cdot): \reals \to \Delta^{[-s_i,+s_i]}$
  
\item The aggregate loss is calculated:
  \begin{equation} \label{eqn:ell-discrete}
    \ell(U_i)=\E{\R \sim \state(U_i)}{\learnerM(U_i,\R) \Bias(U_i,\R)}
    \mbox{ where } \Bias(U_i,\R) \doteq \E{y \sim \adversM(U_i,\R)}{y}
  \end{equation}
  Such that $|\ell(U_i)| \leq s_i^2$
\item The state is updated. 
  $$\state(U_i) = \E{\R \sim \state(U_{i})}{\adversM(U_i,\R) \oplus (\R-\ell(U_i))}
  $$
  Where $\oplus$ is a convolution as defined in the preliminaries.
\end{enumerate}

Upon termination, the final value is calculated:
$$\score(\realT) =\state(\realT) \odot \pot(\realT)$$

\end{minipage}}
\caption{The discrete time potential game  \label{fig:discrete-Time-Game}}
\end{figure}

\subsection{Potential Based analysis} \label{sec:potentialRecursion}
Next,we vary the strategies of one side or the other to define upper
and lower potentials.
\begin{equation} \label{eqn:upperPotentials}
  \exists \learnerM, \;\;\; \forall \adversM,\;\; \forall 1\leq i \leq
  T,\;\; \forall \R \in \reals,\;\; \upperpot(i,\R) \geq \potPQ(i,R)
\end{equation}
\begin{equation} \label{eqn:lowerPotentials}
  \exists \adversM, \;\;\; \forall \learnerM, \;\; \;\; \forall 1\leq i \leq
  T,\;\; \forall \R \in \reals,\;\; \lowerpot(i,\R) \leq \potPQ(i,R)
\end{equation}

In words, $\upperpot$ is an upper bound on the potential that is 
guaranteed by the learner strategy $\learnerM$ while $\lowerpotd$
is a lower bound that is guaranteed by the adversarial
strategy $\adversM$.

Following the same argument as the one leading to
Theorem~\ref{thm:backward-recursion}. We define upper and lower scores
$\upperscoreM(i),\lowerscoreM(i)$ such that
\begin{equation} \label{eqn:upper-recursion}
  \state_{\learnerM}(T)\odot \finalPotT=\upperscoreM(T)=\upperscoreM(T-1)=\cdots=\upperscoreM(0)=\upperpot(0,0)
\end{equation}
and
\begin{equation} \label{eqn:lower-recursion}
  \state_{\adversM}(T)\odot \finalPotT=\lowerscoreM(T)=\lowerscoreM(T-1)=\cdots=\lowerscoreM(0)=\lowerpot(0,0)
\end{equation}

Our ultimate goal is to find strategies $\learnerM$ and
$\adversM$ such that
\begin{equation} \label{eqn:limitPotential}
\forall i,\R,\;\;\; \lowerpot(i,\R) = \upperpot(i,\R)
\end{equation}
in particular, $\lowerscoreM(0)=\lowerpot(0,0) =
\upperpot(0,0)=\upperscoreM(0)$. This means that
$\adversM,\learnerM$ are a min/max pair of strategies and that
$\lowerscoreM(0)=\upperscoreM(0)$ define the min/max value of the game.
~\\~\\
We do not achieve this for the integer game described in the next
section. To achieve min/max optimality we extend the integer time game
to the discrete time game (section~\ref{sec:discrete}) and to the
continuous time game (\ref{sec:continuous}).

{\bf Justification of bounding average loss} Suppose the game is
played for $T$ iterations and that the adversary uses the strategy
$\adversMd\brac{s,\frac{1}{2}+\epsilon}\paren{t,\R}$ and that
$s=\frac{1}{\sqrt{T}}$. In this case the loss of the learner in
iteration $i$ is $\ell(i)=2s\epsilon$ and the total loss is
$$L_\ell^T=\sum_{i=0}^{T-1} \ell(i) = T 2 \epsilon s = \frac{2 \epsilon}{s}$$.
If $\epsilon$ is kept constant as $s \to 0$ then
$\lim_{T \to \infty}L_\ell^T=\infty$, biasing the game towards the
adversary. On the other hand, if $\epsilon =s^{\alpha}$ for $\alpha<1$
then $L_\ell^T \to 0$, biasing the game towards the learner. To keep
the game balanced we have to set $\epsilon=cs$ for some constant
$c$. Without loss of generality we set $c=1$.
Generalizing this to the game where the adversary can choose a
different $s_i$ in each iteration we get the constraint
$|\ell(i)| \leq s_i^2$

\subsection{Strategies for discrete time}
\label{sec:disc-game-strategies}

We fix a real number $\realT$ as the real length of the game.

We define a sequence of adversarial strategies, indexed by $k$, where the step size of $\adversMdk$ is $s_k = 2^{-2k} \sqrt{\realT}$.

We define a sequence of adversarial strategies $\adversMdk$ and
matching learner strategies $\learnerMdk$ for $k=0,1,2,\ldots$. The
adversarial strategies are designed so that the upper and lower
potentials converge to a limit as $k \to \infty$.

We set the time points $t_i =i s_k^2$ for $i=0,1,\ldots,2^{2k}$. We
call the resulting games $k$-discrete and denote them as $D(k)$.

For a given $k$ we define upper and lower potentials for each
$t_i$. This is done by induction starting with the final potential
function  $\finalPotR(\R)=\lowerpotMdk(\realT, \R)=\upperpotMdk(\realT, \R)$ and iterating
backwards for  $i=T,T-1,\ldots,0$, $t_i = i s_k^2$ 
 \begin{equation} \label{eqn:backward-iteration-lower-discrete}
   \lowerpotMdk(t_{i-1}, \R) = \frac{\lowerpotMdk(t_i,\R+s_k) + \lowerpotMdk(t_i,\R-s_k)}{2}
 \end{equation}

 \begin{equation} \label{eqn:backward-iteration-upper-recursion-discrete}
   \upperpotMdk(t_{i-1}, \R) = \frac{\upperpotMdk(t_i,\R+s_k(1+s_k)) + \upperpotMdk(t_i,\R-s_k(1+s_k))}{2}
 \end{equation}

 
These upper and lower potentials correspond to strategies for the
adversary and the learner.
The adversarial strategy is 
\begin{equation} \label{eqn:adv-strat-dk}
  \adversMdk=  \begin{cases}
    +s_k & \mbox{ w.p. } \frac{1}{2}\\
    -s_k & \mbox{ w.p. } \frac{1}{2}\\
  \end{cases}
\end{equation}

The learner's strategy is:
\begin{eqnarray} \label{eqn:learner-strat-1c}
  \learnerMdk(t_{i},\R) = \frac{1}{Z}
  \frac{\upperpotMdk(t_{i+1},\R+s_k(1+s_k)) -
  \upperpotMdk(t_{i+1},\R-s_k(1+s_k))}{2} \\
  \mbox{ where } Z = \E{\R \sim \state(t_{i+1})}{\frac{\upperpotMdk(t_{i+1},\R+s_k(1+s_k)) -
  \upperpotMdk(t_{i+1},\R-s_k(1+s_k)}{2}} \nonumber
\end{eqnarray}

The potentials and strategies defined above are scaled versions of the
integer time potential recursions defined in
Equations~(\ref{eqn:backward-iteration-lower},\ref{eqn:backward-iteration-upper-recursion})
and the strategies defined in Equations~(\ref{eqn:adv-strat-p},\ref{eqn:learner-strat-1}). Specifically, the games operate on lattices that we will now describe.

The adversarial strategy $\adversMb$ defines the following lattice over $i$ and $R$:
$$\Ilat{T}=\left\{ (i,2j-i) \left| 0 \leq i \leq T, 0 \leq j \leq i\right. \right\}$$

The $k$'th adversarial strategy $\adversMdk$ uses step size $s_k=\sqrt{\realT} 2^{-k}$ and time
increments $s_k^2=\realT 2^{-2k}$. We define the {\em game lattice}
for $k$ as the set of $(t,R)$ pairs that are reached by $\adversMdk$.
\begin{equation}  \label{eqn:game-lattice}
  \Klat{{\realT,k}}=\left\{ (t,\R) \left| t=i s_k^2, 0 \leq i \leq 2^{2k}, \R=(2j-i)s_k, 0 \leq j \leq i\right. \right\}
  \end{equation}
$\Ilat{T}$ is a special case of $\Klat{\realT,k}$ because setting
$\realT=T=2^{2k}$ we get that $s_k=s_k^2=1$ and  $\Klat{\realT,k} = \Ilat{T}$.

It is not hard to show that the lattices get finer with $k$, i.e. if  $j \leq k$,  $\Klat{\realT,j} \subseteq \Klat{\realT,k}$.

The following Lemma parallels Lemma~\ref{lemma:first-order-bound} for the integer time game.
\begin{lemma} \label{lemma:discrete-step-bound}
~\\
Let $i$ be an integer between $1$ and $T$

If $\lowerpotMdk(t_i,\R) \in \SP{2}$
\begin{enumerate}
\item  $\lowerpotMdk(t_{i-1},\R) \in \SP{2}$
\item The adversarial strategy~(\ref{eqn:adv-strat-dk})
  guarantees the recursion given in Eq.~(\ref{eqn:backward-iteration-lower-discrete})
\end{enumerate}

If $\upperpotMdk(t_i,\R) \in \SP{2}$
\begin{enumerate}
\item $\upperpotMdk(t_{i-1},\R) \in \SP{2}$
\item The learner strategy~(\ref{eqn:learner-strat-1c})
  guarantees the recursion given in Eq.~(\ref{eqn:backward-iteration-upper-recursion-discrete})
\end{enumerate}

\end{lemma}
\proof
The statement of the Lemma and the proof are scaled versions of
Lemma~\ref{lemma:first-order-bound} and its proof. The iteration step
is $s_k^2$ instead of $1$ while the loss/gain of an action in a single
step is $[-s_k,s_k]$ instead of $[-1,+1]$.

One change worth noting is at the step from
Equation~(\ref{eqn:Pot-Update}) and Equation~(\ref{eqn:pot-upper}),
where the bound  $y-\ell(i) \in [-2,2]$ is replace by  $y-\ell(i) \in
[-s_k-s_k^2,s_k+s_k^2]$. This follows from the bound $|\ell(i)| \leq
s_k^2$ which is discussed in Section~\ref{sec:discrete}.
\qed

\begin{theorem} \label{thm:DescreteGameExactValues}
  Let $\finalPotR \in \SP{2}$ be the final potential in the discrete
  time game. Fix $k$ and the step size $s_k=\sqrt{\realT} 2^{-k}$, and let $t_i=i s_k^2$ for $i=0,1,\ldots,2^{2k}$ and
  let $\R_0$ be a real value, then 
  \begin{itemize}
  \item
    The lower potential guaranteed by $\adversMdk$ is
    \begin{equation} \label{eqn:lower-potential-exact}
      \lowerpotMdk(t_i,\R_0) = \E{\R \sim \R_0 \oplus
        \Binom\paren{2^{2k}-i,s_k}}{\finalPotR(\R)}
      \end{equation}
  \item
    The upper potential guaranteed by $\learnerMdk$ is
    \begin{equation} \label{eqn:upper-potential-exact}
    \upperpotMdk(t_i,\R_0) =  \E{\R \sim \R_0 \oplus
      \Binom\paren{2^{2k}-i,s_k(1+s_k)}}{\finalPotR(\R)}
    \end{equation}
  \end{itemize}
\end{theorem}

Using Theorem~\ref{thm:DescreteGameExactValues} we can show that, as
$k \to \infty$, the upper lower potential converge to the same limit.

\begin{theorem} \label{thm:seq-of-adv-strategies}
  ~\\
  Fix $\realT$ and assume $\finalPotR \in \SP{2}$. Consider the sequence of upper
  and lower potentials  $\upperpotMdk,\lowerpotMdk$ for
  $k=0,1,2,\ldots$.

  Then for any  $0 < t \leq \realT$ and any $\R_0$:
  \begin{equation} \label{eqn:k-limit}
    \lim_{k \to \infty} \upperpotMdk(t,\R_0) =
    \lim_{k \to \infty}\lowerpotMdk(t,\R_0)=
    {\cal N}(\R_0,\realT-t) \odot \finalPotR
  \end{equation}
\end{theorem}

\proof

We first assume that $(t,\R_0) \in \Klat{j,\realT}$ and that $k\geq j$. We later expand to any $0 < t \leq \realT$ and any $\R_0 \in \reals$.
Consider Equation~\ref{eqn:upper-potential-exact} for $\learnerMdk$ and $\learnerMdj$,
keeping $t$ and $j$ constant and letting $k \to \infty$.

\begin{equation} \label{eqn:upper-potential-exact-j}
  \upperpotMdj(t,\R_0) =  \E{\R \sim \R_0 \oplus
    \Binom\paren{2^{2j}-i_j,s_j(1+s_j)}}{\finalPotR(\R)}
\end{equation}

\begin{equation} \label{eqn:upper-potential-exact-k}
  \upperpotMdk(t,\R_0) =  \E{\R \sim \R_0 \oplus
    \Binom\paren{2^{2k}-i_k,s_k(1+s_k)}}{\finalPotR(\R)}
\end{equation}
We rewrite the binomial factor in
Eq~(\ref{eqn:upper-potential-exact-k})
$$
 \Binom\paren{2^{2k}-i_k,s_k(1+s_k)} =
 \Binom\paren{2^{2(k-j)}\paren{2^{2j}-i_j},2^{j-k} s_j(1+2^{j-k} s_j)}
 $$
As $j$ is constant, $s_j$ is constant and so is $a_j \doteq
2^{2j}-i_j$. Multiplying the number of steps by the variance per step
we get
$$Var(\Binom_k) = 2^{2(k-j)}a_j\paren{2^{j-k} s_j(1+(2^{j-k} s_j)}^2
= a_j s_j^2 (1+(2^{j-k} s_j))^2 
$$

As $s_j,a_j$ are constants we get that $\lim_{k \to \infty}
Var(\Binom_k) = a_j s_j$.  From the central limit theorem we get that
for any $(t,\R_0) \in \Klat{j,\realT}$
\begin{equation} \label{eqn:conergence-for-k}
  \lim_{k \to \infty} \upperpotMdk(t,\R_0) \odot \finalPotR = {\cal
    N}(\R_0,\realT-t) \odot \finalPotR
\end{equation}

Our argument hold for all $(t,\R_0) \in \bigcup_{k=0}^\infty \Klat{k,\realT}$ which is dense in the set $0<t\leq \realT, \R_0 \in \reals$.
On the other hand, $\upperpotMdk(t,\R) \odot \finalPotR$ is continuous in both
$t$ and $R$, therefor Equation~(\ref{eqn:conergence-for-k})  holds for all $t$ and $\R$.

As similar (slightly simpler) argsument holds for the lower potential limit
$\lim_{k \to \infty} \lowerpotMdk(t,\R_0)$

\qed

We have shown that in the limit $s \to 0$ the learner and the
adversary converge to the same potential function. In the next section
we show that this limit is the min/max solution by describing conditions
under which the adversary prefers using ever smaller steps size.

\subsection{The adversary prefers smaller steps} \label{sec:smallsteps}

Theorem~\ref{eqn:conergence-for-k} characterizes the limits of the
upper and lower potentials, as $k \to \infty$ are equal to each other
and to ${\cal N}(\R_0,\realT-t) \odot \finalPotR$ To show that this
limit corresponds to the min/max slolution of the game we need to show
that the adversary perfers smaller steps. In other words, that
for any $t,R$, $\lowerpotMdk(t,\R)$ increases with $k$.

To prove this claim we strengther the condition $\finalPotR \in \SP{2}$ used above to $\finalPotR \in \sp{4}$. In words, we assume that the function $\finalPotR(\R)$ is continuous and strictly positive and it's first four derivatives are continuous and strictly positive.

We use the sequence of discrete adversarial strategies
$\adversMdk, k=1,2,\ldots$ defined in
Section~\ref{sec:disc-game-strategies}.

\begin{theorem}\label{thm:smallerSteps}
  If $ \finalPotR \in \SP{4}$, and $\realT>0$  
  then for any $k\geq 0$, any $t \in [0,\realT]$ and any $\R$
  $$\lowerpotMdkpar{k+1}(t,\R) >  \lowerpotMdkpar{k}(t,\R)$$
\end{theorem}

The proof of the theorem relies on a reduction to a simpler case:
dividing a single time step of duration $\tau$ into four time steps of duration  $\tau/4$

\begin{lemma} \label{lemma:half-step}
  If $ \finalPotTau \in \SP4$, and $\tau>0$ then for any $\R$
  $$\lowerpotMdkpar{1}(0,\R) >  \lowerpotMdkpar{0}(0,\R)$$
\end{lemma}
  
\proof The step sizes for $k=0,1$ are
$s_0=\sqrt{\tau}, s_1=\frac{\sqrt{\tau}}{2}$ and the time icrements
are $\Delta t_0=\tau, \Delta t_1=\frac{\tau}{4}$. 
In other words $k=0$ corresponds to a single step of size $\sqrt{\tau}$, while $k=1$ corresponds
to {\em four} steps of size $\frac{\sqrt{\tau}}{2}$.

By definition $\finalPotTau(R)=\lowerpotMdkpar{0}(\tau,\R)=\lowerpotMdkpar{1}(\tau,\R)$

For $k=0$ we we get the recursion
\begin{equation}  \label{eqn:pot-recursion-0}
  \lowerpotMdkpar{0}(0, \R) =
  \frac{\lowerpotMdkpar{0}(\tau,\R-\sqrt{\tau})+
    \lowerpotMdkpar{0}(\tau,\R+\sqrt{\tau})}{2}
  =   \frac{\finalPotTau(\R-\sqrt{\tau})+
    \finalPotTau(\R+\sqrt{\tau})}{2}
\end{equation}

For $k=1$ we we have for $i=0,1,2,3$:
\begin{equation}   \label{eqn:lowerpotquarterstep}
   \lowerpotMdkpar{0}\paren{\frac{i}{4}\tau,\R}=
 \frac{\lowerpotMdkpar{0}\paren{\frac{i+1}{4}\tau,\R-\frac{1}{2}\sqrt{\tau}}+
   \lowerpotMdkpar{0}\paren{\frac{i+1}{4}\tau,\R+\frac{1}{2}\sqrt{\tau}}}{2}
 \end{equation}

\newcommand{\iter}[1]{\lowerpotMdkpar{1}\paren{\tau,\R {#1} \sqrt{\tau}}}
\newcommand{\iterzero}{\lowerpotMdkpar{1}\paren{\tau,\R}}

\newcommand{\fIter}[1]{\finalPotTau \paren{R {#1} \sqrt{\tau}}}
\newcommand{\fIterzero}{\finalPotTau \paren{R}}

\newcommand{\gIter}[1]{\finalPotTau \paren{R {#1} a}}
\newcommand{\gIterzero}{\finalPotTau \paren{R}}

Combining Equation~(\ref{eqn:lowerpotquarterstep}) for $k=0,1,2,3$ we get
\small
\begin{eqnarray} \label{eqn:pot-recursion-1}
  \lowerpotMdkpar{1}(0, \R)
  &=& \frac{1}{16}
      \left[\iter{-2}+4\iter{-} \right. \\
  &&+ \left. 6\iterzero +4\iter{+}+\iter{+2}\right] \nonumber\\
  &=& \frac{1}{16}
      \left[\fIter{-2}+4\fIter{-}+6\fIterzero +4\fIter{+}+\fIter{+2}\right] \nonumber
\end{eqnarray}
\normalsize the difference between
Equations~(\ref{eqn:pot-recursion-1}) and~(\ref{eqn:pot-recursion-0})
is \small
\begin{eqnarray} \label{eqn:pot-recursion-diff}
  \lefteqn{\lowerpotMdkpar{1}(0, \R) - \lowerpotMdkpar{0}(0, \R)}\\
&=&  \frac{1}{16}
\left[\fIter{-2}-4\fIter{-}+6\fIterzero -4\fIter{+}+\fIter{+2}\right] \nonumber
\end{eqnarray}
\normalsize Our goal is to show that the LHS of
Eqn.~\ref{eqn:pot-recursion-diff} is positive. This is equivalent to
proving positivity of
\begin{eqnarray}
g_a(\R) &=& \frac{2}{3a^2}\paren{\lowerpotMdkpar{1}(0, \R) -
  \lowerpotMdkpar{0}(0, \R)}\nonumber \\
& = &
\frac{1}{24 a^4}
      \left[\gIter{-2}-4\gIter{-}+6\gIterzero -4\gIter{+}+\gIter{+2}\right]
       \label{eqn:recursion-as-difference}
\end{eqnarray}
where $a=2 s_1=\sqrt{\tau}$

The function $g_a(\R)$ has a special form called ``divided
differences''. The proof of the following lemma uses this fact to show that 
Eqn~(\ref{eqn:recursion-as-difference}) is strictly positive.
\begin{lemma} \label{lemma:divdiff}
If $\finalPotTau \in \SP{4}$ and $\tau>0$, then $\forall \R, g_a(\R)>0$
\end{lemma}
The proof of Lemma~\ref{lemma:divdiff} is given in appendix~\ref{sec:divdiff}

\proof  of Theorem~\ref{thm:smallerSteps} \\

Let $ \Klat{{\realT,k}}$ be the game lattices defined in
Equation~\ref{eqn:game-lattice}. The statement of
Lemma~\ref{lemma:half-step} can be trivially generalized as follows. Fix $k\geq 0$ and $t=i s_k^2$
then 
\begin{equation} \label{eqn:generalized-half-step}
  \mbox{ if } \phi((i+1) s_k^2,\R) \in \SP{4} \mbox{ then }
  \lowerpotMdkpar{k+1}(i s_k^2,\R) >  \lowerpotMdkpar{k}(i s_k^2,\R)
\end{equation}
The proof of the theorem is by double induction, first on $k=0,1,2,\ldots$ and then on
$t=\tau-s_k^2,\tau-2s_k^2,\ldots,0$.

The base case $k=0,t=\tau$ follows directly from Lemma ~\ref{lemma:half-step}.

For $k>0$ we use a second induction $t=\tau-s_{k-1}^2,\tau-2s_{k-1}^2,\ldots,0$, which, combined with 
~\ref{eqn:generalized-half-step} holds for $k-1$, shows that for any $(t,\R) \in \Klat{{\realT,k-1}}$
we have $ \lowerpotMdkpar{k}(t,\R) >  \lowerpotMdkpar{k-1}(t,\R)$

% we take a finite backward induction over
% $t=T-2^{-2k},T-2 \times 2^{-2k},T-3 \times 2^{-2k},\cdots,0$.
% Our inductive claims are that $\pot_{k+1}(t,\R) > \pot_{k}(t,\R)$ and
% $\pot_{k+1}(t,\R)$,$\pot_{k}(t,\R)$ are in $SP{4}$. That these claims carry over
% from $t=T-i \times 2^{-2k}$ to  $t=T-(i+1) \times 2^{-2k}$ follows
% directly from Lemma~\ref{lemma:n-strictly-convex}.

The theorem follows by forward induction on $k$.
\qed

Combining Theorems~\ref{thm:seq-of-adv-strategies}
and~\ref{thm:smallerSteps} we get our characterization of the min/max
solution for the potential game.

\begin{theorem} \label{thm:min-max-limit}
Let $\realT>0$ and $\finalPotR \in \SP{4}$, define $\phi^*(t,\R) = {\cal N}(\R_0,\realT-t) \odot \finalPotR$
Fix a state at time $t<\realT$ $\state(t)$ and define the score of this state
to be $\score(t) = \state(t) \odot \pot(t)$. Then for any $\epsilon>0$ there exists
\begin{itemize}
\item A learner strategy that guarantees $\score(T) \odot \finalPotR \leq \score(t)+\epsilon$ against any adversarial strategy and
  \item an adversarial strategy that guarantees  $\score(T) \odot \finalPotR \geq \score(t)-\epsilon$
\end{itemize}
\end{theorem}
\proof
From Theorem~\ref{thm:seq-of-adv-strategies} we know that the  $k \to \infty$.
From Theorem~\ref{thm:smallerSteps} we know that the small step limit maximizes the 

Together these theorems show that the the min/max optimal potential function is $ {\cal N}(\R_0,\realT-t) \odot \finalPotR$.

\section{Characterizing the limit $s \to 0$}
\label{sec:continuous}

There seems to be a paradox: the adversary prefers to set $s_i>0$ as
small as possible. On the other hand, there is no minimal strictly
positive number. Whatever step size the adversary chooses, it regrets
not choosing an even smaller step!

Luckily, this paradox does not bother the learner. The learner wants
to protect itself from an adversary with an arbitrarily small step
size. To achieve that, it just needs to use the limit potential function for $s \to 0$.

Consider the distribution of regret sequences generated by the adversary XXX. If we index these sequences by $V_t = ts^2$ and let $s \to 0$ we arrive at the well known Brownian or Wiener process
(see~\cite{kac1947random}).

%An alternative characterization of Brownian Process is
%$$ \P{}{X_{t+\deltat}=x_1 | X_t=x_0}=e^{-\frac{(x_1-x_0)^2}{2 \deltat}}$$

The backwards recursion that defines the min-max potential function
for intermediate steps is the celebrated Backwards Kolmogorov Equation
with no drift and unit variance
\begin{equation} \label{eqn:Kolmogorov}
  \frac{\partial}{\partial t} \pot(t,\R)
  + \frac{1}{2} \frac{\partial^2}{\partial \R^2} \pot(t,\R)=0
\end{equation}
Given a final value function with a strictly positive fourth
derivative we can use Equation~(\ref{eqn:Kolmogorov}) to compute the
value function for all $0 \leq t \leq T$. 

The results up to this point hold for any potential function in
$\SP{4}$. Given a final potential function $\finalPotR \in \SP{4}$ we
can compute the potential for any $0 \leq t \leq \realT$ and any $R$ using the equation 
\begin{equation} \label{eqn:convol-with-normal}
\pot(t,\R)={\cal N}(\R_0,\realT-t) \odot \finalPotR
\end{equation}
By using uses the $\R$-derivative of this potential function to define
the weights the learner guarantees that the final average score is at
most $\pot(0,0)$.

A major limition of this result is that the horizon $\realT$ is set in
advance.  It is desirable that the potential is defined without
knowledge of the horizon.  In what follows we show that Hedge and
NormalHedge can both be used in such ``anytime'' algorithms.

Our solution is based on the observation that a potential function satisfies Eqn~(\ref{eqn:convol-with-normal}) if and only if it satisfies 
the Kolmogorov backwards PDE~(\ref{eqn:Kolmogorov}):
The potential $\finalPotR \in \SP{4}$ defines a boundary condition of the PDE.

We derive our anytime algorithm by finding solutions to the Kolmogorov
PDE that are not restricted in time, and that have a fixed parametric
form.  In other words, the evolution of the potential with time is
defined by changing the parameter values, without changing the form.

We describe two potential functions that are solutions of PDE. In the following section we use our general results to prove simultanous regret bounds

{\bf The exponential potential function} which corresponds to exponential
  weights algorithm corresponds to the following equation
\begin{equation} \label{eqn:exponential-potential}
    \pot_{\mbox{\tiny exp}}(\R,t) = e^{\sqrt{2} \eta \R - \eta^2 t}
\end{equation}

Equation~\ref{eqn:exponential-potential}.

For the standard (non simultanous) bound we fix $\epsilon$ and $t$,
choose $\eta = \sqrt{\frac{\ln (1/\epsilon)}{t}}$
and get a bound of the form 
  \begin{equation}
    \R_\epsilon \leq \sqrt{2 t \ln \frac{1}{\epsilon}}
  \end{equation}

  To derive a simultanous regret bound we fix the learning rate $\eta$ and take the reciprocal of the potential:
 \[
    G(R,t) \leq e^{\eta^2 t - \sqrt{2}\eta R}
 \]
Which holds for any $t$ and $R$. The bound $G(R,t)$ depends on $\eta$.

  
{\bf The NormalHedge potential function} parametrized by $\nu>0$ is:
\begin{equation} \label{eqn:NormalHedge}
  \pot_{\mbox{\tiny NH$(\nu)$}}(\R,t) = \begin{cases}
    \frac{1}{\sqrt{t+\nu}}\exp\left(\frac{\R^2}{2(t+\nu)}\right)
    & \mbox{if } \R \geq 0  \\
  \frac{1}{\sqrt{t+\nu}} & \mbox{if } \R <0
  \end{cases}
\end{equation}

The function $\pot_{\mbox{NH}}(\R,t)$ is not in $\SP{4}$, however, the
positive part $\R\geq 0$ is in $\SP{4}$ while the negative part
$\R \leq 0$ is a constant. A constant potential corresponds to zero weight which means that actions whose regret is negative are ignored by the learner. In this case the optimal adversarial is not unconstrained brownian motion, instead it is brownian motion with a reflective boundary at $R=0$.

\section{The continuous time game and bounds for easy
  sequences} \label{sec:easy}

In Section~\ref{sec:discrete} we have shown that the integer time game
has a natural extension to a setting where $\deltat_i = s_i^2$. We
also demonstrated sequences of adversarial strategies $S_1,S_2,\ldots$
such that $\sup_{k \to \infty} {\lowerpot}_k(0,\R) = $

We characterized the optimal adversarial strategy for the discrete
time game (Section~\ref{sec:discrete-Time-Game}), which corresponds
to the adversary choosing the loss to be $s_i$ or $-s_i$ with equal
probabilities. A natural question at this point is to characterize the
regret when the adversary is not optimal, or the sequences are ``easy''.

To see that such an improvement is possible, consider the following
{\em constant} adversary. This adversary associates the same loss to
all actions on iteration $i$, formally, $\adversM(i,\R) = l$. In this
case the average loss is also equal to $l$, $\ell(i)=l$ which means
that all of the instantaneous regrets are $r=l-\ell(t_i) = 0$, which,
in turn, implies that $\state(i) = \state(i+1)$. As the state did not
change, it makes sense to set $t_{i+1}=t_i$, rather than
$t_{i+1}=t_i+s_i^2$.

We observe two extremes for the adversarial behavior. The constant
adversary described above for which $t_{i+1} = t_i$, and the random walk adversary described
earlier, in which each action is split into two, one half with loss
$-s_i$ and the other with loss $+s_i$. In which case $t_{i+1} =
t_i+s_i^2$ which is the maximal increase in $t$ that the adversary can
guarantee. The analysis below shows that these are two extremes on a
spectrum and that intermediate cases can be characterized using a
variance-like quantity.

We define a variant of the discrete time game
(\ref{sec:discrete-Time-Game}) For concreteness we include the
learner's strategy, which is the limit of the strategy in the discrete
game when $s_i \to 0$.

\iffalse
\begin{figure}[ht!]
\framebox{
\begin{minipage}[t]{6.4in}
Set $t_1=0$ \\
Fix maximal step $0<s<1$ \\
On iteration $i=1,2,\ldots$

\begin{enumerate}
\item  If $t_i=T$ the game terminates.
\item Given $t_{i}$, the learner chooses a distribution
  $\learnerM(i)$ over $\reals$:
  \begin{equation} \label{eqn:learner-strat-cc}
  \learnerM^{cc}(t,\R) =  \frac{1}{Z^{cc}}
  \left. \frac{\partial}{\partial r} \right|_{r=\R} \pot(t,r)
  \mbox{ where } Z^{cc} = \E{\R \sim \state(t_i)}{\left. \frac{\partial}{\partial r} \right|_{r=\R} \pot(t,r)}
\end{equation}

\item The adversary chooses a {\em step size} $0<s_i\leq s$ and a mapping from $\reals$ to distributions
  over $[-s_i,+s_i]$: $\adversM(t): \reals \to \Delta^{[-s_i,+s_i]}$
\item The aggregate loss is calculated:
  \begin{equation} %\label{eqn:ell-discrete}
    \ell(t_i)=\E{\R \sim \state(t_i)}{\learnerM^{cc}(t_i,\R)
      \Bias(t_i,\R)},\;\mbox{ where } \Bias(t_i,\R) \doteq \E{y \sim \adversM(t_i,\R)}{y}
  \end{equation}
  the aggregate loss is restricted to $|\ell(t_i)| \leq c s_i^2$.
\item  Increment $t_{i+1} = t_{i} + \deltat_i$ where
\begin{equation} \label{eqn:deltat}
  \deltat_i=
  \E{\R \sim \state(t_i)}{H(t_i,\R) \;\; \E{y \sim \adversM(t_i,\R)}{(y-\ell(t_i))^2}}
\end{equation}
Where
\begin{equation}
 H(t_i,R)=\frac{1}{Z^H} \left. \frac{\partial^2}{\partial r^2} \right|_{r=\R} \pot(t_i,r)
  \mbox{ and } Z^H = \E{\R \sim \state(t_i)}{\left. \frac{\partial^2}{\partial r^2} \right|_{r=\R} \pot(t_i,r)}
\end{equation}

\item The state is updated.
  $$\state(t_{i+1}) = \E{\R \sim \state(t_{i})}{\adversM(t_i)(\R)\oplus (\R-\ell(t_i))}
  $$
\end{enumerate}
\end{minipage}}
\caption{The continuous time game and learner strategy\label{sec:contin-Time-Game}}
\end{figure}



Our characterization applies to the limit where the $s_i$ are small. Formally, we define
\begin{definition}
We say that an instance of the discrete time game is
$(n,s,\tau)$-bounded if it consists of $n$ iterations and $\forall\;\; 0<i\leq n,\;\; s_i < s$ and $\sum_{j=1}^n s_j^2=\tau$
\end{definition}

Note that $\tau>t_n$ and that $\tau$ depends only on the ranges $s_i$
while $t_n$ depends on the variance. $t_n = T$ 
is the dominant term in the regret bound, while $\tau$ controls the
error term.

\fi

\begin{theorem} \label{thm:variancebound} Let $\pot \in \SP{\infty}$
  be a potential function that satisfies the Kolmogorov backward
  equation~(\ref{eqn:Kolmogorov})

Then 
$$\score(\state(\tau)) \leq \score(\state(0))$$
\end{theorem}

The proof is given in appendix~\ref{appendix:ProofOfVarianceBound}

If we define

\begin{equation} \label{eqn:Vn}
  V_n = t_n = \sum_{i=1}^n \deltat_i= 
  \sum_{i=1}^n \E{\R \sim \state(i)}{\E{y \sim \adversM(t_i,\R)}{H(t_i,\R) ((y-\ell_i)^2)}}
\end{equation}

We can use $V_n$ instead of $T$ giving us a variance based bound.

  
%    \E{\R \sim \state(i)}{ \E{y \sim
%      \adversM(i)(\R)}{\pot(i,\R+y-\ell(i))}}

\section{Lower bound} \label{sec:lowerbound}

We prove the lower bound using Lyapunov CLT ~\cite{billingsley1995central}


\bibliographystyle{plain}
\bibliography{ref.bib,bib.bib}

\appendix
\section{Proof of Theorem~\ref{thm:IntegerGameBounds}}


The following lemma states that these strategies guarantee the
corresponding potentials.
\begin{lemma} \label{lemma:first-order-bound}
~\\
Let $i$ be an integer between $1$ and $T$

If $\lowerpotb(i,\R) \in \SP{2}$
\begin{enumerate}
\item {\bf Positivity:} $\lowerpotb(i-1,\R) \in \SP{2}$
\item {\bf Adversary:} The adversarial strategy~(\ref{eqn:adv-strat-p})
  guarantees the recursion given in Eq.~(\ref{eqn:backward-iteration-lower})
\end{enumerate}

If $\upperpotb(i,\R) \in \SP{2}$
\begin{enumerate}
\item {\bf Positivity:} $\upperpotb(i-1,\R) \in \SP{2}$
\item {\bf Learner:} The learner strategy~(\ref{eqn:learner-strat-1})
  guarantees the recursion given in Eq.~(\ref{eqn:backward-iteration-upper-recursion})
\end{enumerate}

\end{lemma}

\proof {\bf Of Lemma~\ref{lemma:first-order-bound}

    We prove each claim in turn
\begin{enumerate}
\item {\bf Positivity:} Follows from Lemma~\ref{lemma:SP-pos-comb}.
\item{\bf Adversary:} By symmetry adversarial strategy~(\ref{eqn:adv-strat-p}) guarantees that
  the aggregate loss~(\ref{eqn:aggregate-loss}) is zero regardless of
  the choice of the learner: $\ell(i)=0$.
  Therefor the state update~(\ref{eqn:state-update}) is equivalent to
  the symmetric random walk:
  $$\state(i) = \frac{1}{2} \paren{(\state(i) \oplus 1) + (\state(i)
    \ominus 1)}$$
  Which in turn implies that if the adversary plays $\adversM^*$
  and the learner plays an arbitrary strategy $\learnerM$
  \begin{equation} \label{eqn:lower}
    \lowerpotb(i-1,\R) = \frac{\lowerpotb(i,\R-1)+\lowerpotb(i,\R+1)}{2}
  \end{equation}
  As this adversarial strategy is oblivious to the learner's strategy, it
  guarantees that the average value at iteration $i$ is {\em equal} to the
  average of the lower value at iteration $i$.
\item {\bf Learner:}
  Plugging learner's strategy~(\ref{eqn:learner-strat-1})
  into equation~(\ref{eqn:aggregate-loss}) we find that
 \begin{equation} \label{eqn:ell-optimal-learner}
   \ell(i) = \frac{1}{Z_{i}} \E{\R \sim \state(i)}{\paren{\upperpotb(i,\R+2)-\upperpotb(i,\R-2)}
   \Bias(i,\R)}
\end{equation}
  Consider the score at iteration $i$ when the learner's strategy
  is $\learnerM^*$ and the adversarial strategy  $\adversM$ is arbitrary
     \begin{equation} \label{eqn:Pot-Update}
    \score_{\learnerM^*,\adversM}(i,\R) = \E{\R \sim \state(i)}{ \E{y \sim
      \adversM(i)(\R)}{\pot(i,\R+y-\ell(i))}}
  \end{equation}
  As $\pot(i,\cdot)$ is convex and as $y-\ell(i) \in [-2,2]$,
  \begin{equation} \label{eqn:pot-upper}
    \upperpotb(i-1,\R+y) \leq \frac{\upperpotb(i,\R+2)+\upperpotb(i,\R-2)}{2} +
    (y-\ell(i)) \frac{\upperpotb(i,\R+2)-\upperpotb(i,\R-2)}{2}
    \end{equation}
  Combining the equations~(\ref{eqn:ell-optimal-learner}) and~(\ref{eqn:Pot-Update}) we find that
  \begin{eqnarray}
  \score_{\learnerM^*,\adversM}(i,\R)&=&\E{\R \sim \state(i)}{\E{y \sim \adversM(i)(\R)}{\upperpotb(i,\R+y-\ell(i))}}\\
  &\leq & \E{\R \sim \state(i)}{\frac{\upperpotb(i,\R+2)+\upperpotb(i,\R-2)}{2}}\\
  &+&
  \E{\R \sim \state(i)}{\E{y \sim \adversM(i)(\R)}{(y-\ell(i)) \frac{\upperpotb(i,\R+2)-\upperpotb(i,\R-2)}{2}}} \label{eqn:zero-term}
  \end{eqnarray}
  
The final step is to show that the term~(\ref{eqn:zero-term}) is equal
to zero. As $\ell(i)$ is a constant with respect to $\R$ and $y$ the
term~(\ref{eqn:zero-term}) can be written as:
\begin{eqnarray}
&&\E{\R \sim \state(i)}{\E{y \sim \adversM(i)(\R)}{(y-\ell(i))
   \frac{\upperpotb(i,\R+2)-\upperpotb(i,\R-2)}{2}}}\\
&=&
\E{\R \sim \state(i)}{\Bias(i,\R)
    \frac{\upperpotb(i,\R+2)-\upperpotb(i,\R-2)}{2}}\\
  &-& \ell(i) \E{\R \sim \state(i)}{
    \frac{\upperpotb(i,\R+2)-\upperpotb(i,\R-2)}{2}}\\
  &=& 0
\end{eqnarray}
\end{enumerate}
\qed

Repeating the induction steps of Lemma~\ref{lemma:first-order-bound}
from $i=T$ to $i=1$ yields the following theorem.

\section{Proof of Theorem~\ref{thm:simulBoundAveragePot} \label{proof:simulBoundAveragePot}}
\proof
~\\
\begin{itemize}
  \item
  {\bf A distribution $\state$ satisfies SRB for $G$ if it satisfies APB
  for $\pot(R) = G(R)^{-1}$}\\
Assume by contradiction that $\state$ does not satisfy the simultaneous bound. In
other words there exists $a \in \reals$ such that
$\P{\R \sim \state}{\R > a} > B(a)$. From Markov inequality and the fact
that $\phi$ is non decreasing we get
\[
  \E{\R \sim \state}{\pot(\R)} \geq \phi(a) \P{\R \sim \state}{\R > a} >
  \phi(a) B(a) = \frac{B(a)}{B(a)}=1
\]
but $ \E{\R \sim \state}{\pot(\R)} >1$ contradicts the average potential
assumption for the potential $\phi(\R) = B(\R)^{-1}$
\item {\bf A distribution $\state$ satisfies an
    APB for $\pot$ if it satisfies SRB for $G$ and $\int_{-\infty}^{\infty}\pot(\R) G(\R) d\R \leq 1$}\\
  From the SRB condition we have
  $$\forall R, \P{\rho \sim \state}{\rho \geq \R} \leq G(\R)$$
  From the condition  $\int_{-\infty}^{\infty}\pot(\R) G(\R) d\R \leq 1$ we get
  $$1 \geq \int_{-\infty}^{\infty}\pot(\R) G(\R) d\R \geq
  \int_{-\infty}^{\infty}\pot(\R) \P{\rho \sim \state}{\rho\geq \R}
  d\R = \E{\R \sim \state}{\pot(\R)}$$
\end{itemize}
\qed

\section{Divided differences of a function} \label{sec:divdiff}

The function $g_s(\R)$ has a special form called ``divided difference''
that has been extensively studied ~\cite{popoviciu1965certaines,butt2016generalization, de2005divided}.
and is closely related to to derivatives of different orders. Using
this connection and the fact that $\pot(\cdot,\R) \in \SP{4}$ we prove
the following lemma:


We conclude that if $\pot(t',\R)$ has a strictly positive fourth
derivative then $\pot_{k+1}(t,\R) > \pot_{k}(t,\R)$ for all $\R$, proving
the first part of the lemma.

The second part of the lemma follows from the fact that
both $\pot_{k+1}(t,\R)$ and $\pot_{k}(t,\R)$ are convex combinations of
$\pot(t,\R)$ and therefor retain their continuity and convexity properties.

A function $\finalPot{}$ that satisfies
inequality~\ref{eqn:4thOrderConvex} is said to be {\em 4'th order convex}
(see details in in~\cite{butt2016generalization}).


Following\cite{butt2016generalization} we give a brief review of
divided differences and of $n$-convexity.

Let $f:[a,b] \to \reals$ be a function from the segment $[a,b]$ to the
reals.

\begin{definition}[$n$'th order divided difference of a function]
  The $n$'th order divided different of a function $f:[a,b] \to
  \reals$ at mutually distinct and ordered points $a \leq x_0 < x_1
  < \cdots < x_n \leq b$
  defined recursively by
  \[ [x_i; f] = f(x_i), \; i \in 0,\ldots n,\]
  \[ [x_0,\ldots,x_n;f] =
    \frac{[x_1,\ldots,x_n;f]-[x_0,\ldots,x_{n-1};f]}{x_n-x_0} \]
\end{definition}

\begin{definition}[$n$-convexity]
 A function $f:[a,b] \to \reals$ is said to be $n$-convex  $n \geq 0$
 if and only if for all choices of $n+1$ distinct points: $a \leq x_0 < x_1
  < \cdots < x_n \leq b$, $[x_0,\ldots,x_n;f]\geq 0$ holds.
\end{definition}
$n$-convexity is has a close connection to the sign of $f^{(n)}$ - the $n$'th
derivative of $f$, this connection was proved in 1965 by
popoviciu~\cite{popoviciu1965certaines}.
\begin{theorem} \label{thm:popo}
If $f^{(n)}$ exists then f is $n$-convex if and only if $f^{(n)}\geq 0$.
\end{theorem}

The next lemma states that the function $g(\R)>0$ as defined in
Equation~(\ref{eqn:divdiff}).

\proof {\bf of Lemma~(\ref{lemma:divdiff})

Fix $t$ and define $f(x) = \pot(t,x)$.
Let $(x_0,x_1,x_2,x_3,x_4)=(\R-2 s,\R-s,\R,\R+s,\R+2s)$

Using this notation we can rewrite $g(\R)$ in the form
\begin{equation}
  h(x_0,x_1,x_2,x_3,x_4) =  \frac{1}{24s^4} \paren{f(x_4)- 4f(x_3)+ 6f(x_2)-
    4f(x_1)+ f(x_0)}
\end{equation}
Is the 4-th order divided difference of $\pot(t,\cdot)$


\begin{enumerate}
\item
$$[x_i;f] = f(x_i)$$
\item
  $$[x_i,x_{i+1};f]=\frac{f(x_{i+1})-f(x_i)}{s}$$
\item
  $$[x_i,x_{i+1},x_{i+2};f] =
  \frac{\frac{f(x_{i+2})-f(x_{i+1})}{s}-\frac{f(x_{i+1})-f(x_i)}{s}}{2s}
  =\frac{f(x_{i+2})-2f(x_{i+1})+f(x_i)}{2 s^2}
  $$
\item
  \begin{eqnarray*}
    [x_i,x_{i+1},x_{i+2},x_{i+3};f]& = &
    \frac{\frac{f(x_{i+3})-2f(x_{i+2})+f(x_{i+1})}{2
    s^2}-\frac{f(x_{i+2})-2f(x_{i+1})+f(x_i)}{2
    s^2}}{3s}\\
    &=& \frac{f(x_{i+3}) -3f(x_{i+2})+3f(x_{i+1})-f(x_i)   }{6 s^3}
  \end{eqnarray*}
\item
  \begin{eqnarray*}
    [x_i,x_{i+1},x_{i+2},x_{i+3},x_{i+4};f]& = &
    \frac{\frac{f(x_{i+4}) -3f(x_{i+3})+3f(x_{i+2})-f(x_{i+1})   }{6 s^3}
    - \frac{f(x_{i+3}) -3f(x_{i+2})+3f(x_{i+1})-f(x_i)   }{6 s^3}}
    {4s}\\
    &=& \frac{f(x_{i+4})-4f(x_{i+3})+6f(x_{i+2})-4f(x_{i+1})+f(x_i)}{24s^4}
  \end{eqnarray*}
\end{enumerate}

\qed
\section{Proof of Theorem~\ref{thm:variancebound}}
\label{appendix:ProofOfVarianceBound}
We start with two technical lemmas
\begin{lemma} \label{lemma:infiniteexpectations}
Let $f(x) \in \SP{2}$, i.e. $f(x), f'(x),f''(x) >0$ for all $x \in
\reals$, let $h(x)$ be a uniformly bounded function: $\forall x,\;\; |h(x)|<1$.
Let $\state$ be a distribution over $\reals$.
If $\E{x \sim \state}{f(x)}$ is well-defined (and finite) , then 
$\E{x \sim \state}{h(x) f'(x)}$ is well defined (and finite) as well.
\end{lemma}
\proof
Assume by contradiction that $\E{x \sim \state}{h(x) f'(x)}$ is
undefined. Define $h^+(x) = \max(0,h(x))$.
As $f'(x)>0$, this implies that either $\E{x \sim \state}{h^+(x)
  f'(x)}=\infty$ or $\E{x \sim \state}{(-h)^+(x) f'(x)}=\infty$ (or both). 

Assume wlog that $\E{x \sim \state}{h^+(x) f'(x)}=\infty$. As
$f'(x)>0$ and $0 \leq h^+(x) \leq 1$ we get that $\E{x \sim
  \state}{f'(x)}=\infty$.
As $f(x+1) \geq f'(x)$ we get that $\E{x \sim
  \state}{f(x)}=\infty$ which is a contradiction.
\qed


\newcommand{\Dx}{\Delta x}
\newcommand{\Dy}{\Delta y}
\begin{lemma} \label{lemma:Taylor2D}
Let $f(x,y)$ be a differentiable function with continuous derivatives
up to degree three. Then
\begin{eqnarray}
  &&f(x_0+\Dx,y_0+\Dy) = f(x_0,y_0)
  + \atI{\frac{\partial}{\partial x}} \Dx 
  + \atI{\frac{\partial}{\partial y}} \Dy \\
  &+&\frac{1}{2} \atI{\frac{\partial^2}{\partial x^2}} \Dx^2
      +\atI{\frac{\partial^2}{\partial x\partial y}} \Dx\Dy
      +\frac{1}{2} \atI{\frac{\partial^2}{\partial y^2}} \Dy^2\\
  &+&\frac{1}{6} \atII{\frac{\partial^3}{\partial x^3}} \Dx^3
      +\frac{1}{2} \atII{\frac{\partial^3}{\partial x^2 \partial y}} \Dx^2\Dy\\
  &&+ \frac{1}{2} \atII{\frac{\partial^3}{\partial x \partial y^2}} \Dx\Dy^2
    + \frac{1}{6} \atII{\frac{\partial^3}{\partial y^3}} \Dy^3
\end{eqnarray}
for some $0\leq t \leq 1$.
\end{lemma}
\proof {\em of Lemma~\ref{lemma:Taylor2D}} 
Let $F:[0,1] \to \reals$ be defined as  $F(t)=f(x(t),y(t))$ where
$x(t) = x_0+t\Dx$ and $y(t)=y_0+t\Dy$. Then $F(0)=f(x_0,y_0)$ and
$F(1)=f(x_0+\Dx,y_0+\Dy)$. It is easy to verify that
$$ \frac{d}{dt}F(t)
=\frac{\partial}{\partial x} f(x(t),y(t))\Dx
+ \frac{\partial}{\partial y} f(x(t),y(t))\Dy
$$
and that in general:
\begin{equation} \label{eqn:d.dn.F}
\frac{d^n}{d t^n} F(t) = \sum_{m=1}^n {n \choose m}
\frac{\partial^n}{\partial x^m \partial y^{n-m}} f(x_0+t \Dx,y_0+t\Dy)
\Dx^m \Dy^{n-m}
\end{equation}
As $f$ has partial derivatives up to degree 3, so does $F$. Using the
Taylor expansion of $F$ and the intermediate point theorem we get that
\begin{equation} \label{eqn:Taylor.F}
  f(x_0+\Dx,y_0+\Dy) = F(1) = F(0)+\frac{d}{dt}F(0)
  +\frac{1}{2}\frac{d^2}{dt^2}F(0)
  +\frac{1}{6}\frac{d^3}{dt^3}F(t')
\end{equation}
Where $0 \leq t' \leq 1$. Using Eq~(\ref{eqn:d.dn.F}) to expand each
term in Eq.~(\ref{eqn:Taylor.F}) completes the proof.
\qed


\proof {\em of Theorem~\ref{thm:variancebound}}\\
We prove the claim by an upper bound on the increase of potential that holds for any iteration $1 \leq i \leq n$:
\begin{equation} \label{proof:onestep}
\score(\state(t_{i+1})) \leq \score(\state(i)) + a s_i^3 \mbox{ for some constant } a>0
\end{equation}
Summing inequality~(\ref{proof:onestep}) over all iterations we get that 
\begin{equation} \label{proof:allsteps}
\score(\state(T)) \leq \score(\state(0)) + c \sum_{i=1}^n s_i^3 \leq 
\score(\state(0)) + a s \sum_{i=1}^n s_i^2 = 
\score(\state(0)) + a s T
\end{equation}
From which the statement of the theorem follows.

We now prove inequality~(\ref{proof:onestep}). 
We use the notation $r=y -\ell(i)$ to denote the instantaneous regret at iteration $i$. 


Applying Lemma~\ref{lemma:Taylor2D} to
$\pot(t_{i+1},\R_{i+1})=\pot(t_i+\deltat_i,\R_i+r_i)$  we get
\begin{eqnarray} 
    \pot(t_i+\deltat_i,\R_i+r_i) & =&  
    \pot(t_i,\R_i)\\
    &+&\at{\frac{\partial}{\partial \rho}} r_i \\
    &+&\at{\frac{\partial}{\partial \tau}}  \deltat_i \\
    &+& \frac{1}{2} \at{\frac{\partial^2}{\partial \rho^2}} r_i^2 \\
    &+& \at{\frac{\partial^2}{\partial r \partial \tau}} r_i \deltat_i \label{term:Taylor_rdt}\\
    &+& \frac{1}{2} \at{\frac{\partial^2}{\partial \tau^2}} \deltat_i^2 \label{term:Taylor_dtsquare}\\
    &+& \frac{1}{6} \att{\frac{\partial^3}{\partial \rho^3}} r_i^3 \label{term:Taylor_r3}\\
    &+& \frac{1}{2} \att{\frac{\partial^3}{\partial \rho^2 \partial \tau}} r_i^2\deltat_i \label{term:Taylor_r2t}\\
    &+& \frac{1}{2} \att{\frac{\partial^3}{\partial \rho \partial \tau^2}} r_i\deltat_i^2 \label{term:Taylor_rt2}\\
    &+& \frac{1}{6} \att{\frac{\partial^3}{\partial \tau^3}} \deltat_i^3 \label{term:Taylor_t3}
\end{eqnarray}
for some $0 \leq g \leq 1$.

By assumption $\pot$ satisfies the Kolmogorov backward equation:
\begin{equation*} 
  \frac{\partial}{\partial \tau} \pot(\tau,\rho)
  = -\frac{1}{2} \frac{\partial^2}{\partial r^2} \pot(\tau,\rho)
\end{equation*}
Combining this equation with the exchangeability of the order of
partial derivative (Clairiaut's Theorem) we can substitute all
partial derivatives with respect to $\tau$ with partial derivatives
with respect to $\rho$ using the following equation.
\[
  \frac{\partial^{n+m}}{\partial \rho^n \partial \tau^m} \pot(\tau,\rho)=
  (-1)^m \frac{\partial^{n+2m}}{\partial \rho^{n+2m}} \pot(\tau,\rho)
\]
Which yields
\begin{eqnarray}
      \pot(t_i+\deltat_i,\R_i+r_i) & =&  
    \pot(t_i,\R_i)\\
    &+& \at{\frac{\partial}{\partial \rho}} r_i \label{term:coll1}\\
    &+& \at{\frac{\partial^2}{\partial \rho^2}} \paren{\frac{r_i^2}{2}-\deltat_i} \label{term:coll2}\\
    &-& \at{\frac{\partial^3}{\partial \rho^3}} r_i \deltat_i \label{term:coll3}\\
    &+& \frac{1}{2} \at{\frac{\partial^4}{\partial \rho^4}} \deltat_i^2 \label{term:coll4}\\
    &+& \frac{1}{6} \att{\frac{\partial^3}{\partial \rho^3}} r_i^3 \label{term:coll5}\\
    &-& \frac{1}{2} \att{\frac{\partial^4}{\partial \rho^4}} r_i^2\deltat_i \label{term:coll6}\\
    &+& \frac{1}{2} \att{\frac{\partial^5}{\partial \rho^5}} r_i\deltat_i^2 \label{term:coll7}\\
    &-& \frac{1}{6} \att{\frac{\partial^6}{\partial \rho^6}} \deltat_i^3 \label{term:coll8}
\end{eqnarray}

  From the assumption that the game is $(n,s,T)$-bounded we get that 
  \begin{enumerate}
  \item $|r_i| \leq s_i +c s_i^2 \leq 2 s_i$
  \item $\deltat_i \leq s_i^2 \leq s^2$
    % \item $\sum_i \deltat_i=T$
  \end{enumerate}

  given these inequalities we can rewrite the second factor in each
  term as follows, where $|h_i(\cdot)|\leq 1$
  \begin{itemize}
  \item {\bf For~(\ref{term:coll1}):}
    $r_i=2s_i\frac{r_i}{2s_i}=2s_ih_1(r_i)$.
  \item {\bf For~(\ref{term:coll2}):}
    $r_i^2 - \frac{1}{2}\deltat_i = 4s_i^2\frac{r_i^2 -
      \frac{1}{2}\deltat_i}{4s_i^2} = 4s_i^2 h_2(r_i,\deltat_i)$
  \item {\bf For~(\ref{term:coll3}):} $r_i \deltat_i = 2s_i^3
    \frac{r_i \deltat_i}{2s_i^3} = 2s_i^3 h_3(r_i,\deltat_i)$
  \item {\bf For~(\ref{term:coll4}):} $\deltat_i^2 =
    s_i^4\frac{\deltat_i^2}{s_i^4} = s_i^3 h_4(\deltat_i)$
  \item {\bf For~(\ref{term:coll5}):} $r_i^3 = 8s_i^3
    \frac{r_i^3}{8s_i^3} = 8s_i^3 h_5(r_i,\deltat_i)$
  \item {\bf For~(\ref{term:coll6}):} $r_i^2 \deltat_i = 4s_i^4
    \frac{r_i^2 \deltat_i}{4s_i^4} = 4s_i^3 h_6(r_i,\deltat_i)$
  \item {\bf For~(\ref{term:coll7}):} $r_i \deltat_i^2 = 2s_i^5
    \frac{r_i \deltat_i^2}{2s_i^5}$
  \item {\bf For~(\ref{term:coll8}):} $\deltat_i^3 = s_i^6 \frac{\deltat_i^3}{s_i^6}$
\end{itemize}
  We therefor get the simplified equation
  
  \begin{eqnarray*} 
     \pot(t_i+\deltat_i,\R_i+r_i) & =&  \pot(t,\R)+\at{\frac{\partial}{\partial r}} r
    + \at{\frac{\partial}{\partial t}} \deltat \\
                                  &+& 
                                      \frac{1}{2}  \at{\frac{\partial^2}{\partial r^2}} r^2\\
                                  &+& \at{\frac{\partial^2}{\partial r \partial t}} r_i \deltat_i \label{term:Taylor_collected_rdt}\\
                                  &+& \frac{1}{6} \at{\frac{\partial^3}{\partial r^3}} r_i^3 \label{term:Taylor_collected_r3}
                                      + O(s^4)
\end{eqnarray*}

and therefor
  \begin{eqnarray} 
     \pot(t_i+\deltat_i,\R+r) &=& \pot(t_i,\R) +
                                  \at{\frac{\partial}{\partial r}} r
                                  \nonumber \\
    &+& \at{\frac{\partial^2}{\partial r^2}} (r^2 - \deltat_i) +
        O(s^3) \label{eqn:Taylor}
\end{eqnarray}

Our next step is to consider the expected value of~(\ref{eqn:Taylor}) wrt $\R \sim \state(i)$,
$y \sim \adversM(t_i,\R)$ for an arbitrary adversarial strategy
$\adversM$.

We will show that the expected potential does not increase:
\begin{equation} \label{eqn:deltatislargeenough}
     \E{\R \sim \state(i)}{ \E{y \sim \adversM(t_i,\R)}{\pot(t_i+\deltat_i,\R+y-\ell_i)}} \leq \E{\R \sim \state(t_i)}{\pot(t_i,\R)}
\end{equation}

Plugging Eq~(\ref{eqn:Taylor}) into the LHS of
Eq~(\ref{eqn:deltatislargeenough}) we get
\begin{eqnarray}
  \lefteqn{\E{\R \sim \state(t_i)}{\E{y \sim \adversM(t_i,\R)}{\pot(t_i+\deltat_i,\R+y-\ell_i)}}} \\
  &=& \E{\R \sim \state(t_i)}{\pot(t_i,\R)} \label{eqn:contin0}\\
  &+& \E{\R \sim \state(t_i)}{\E{y \sim \adversM(t_i,\R)}{\at{\frac{\partial}{\partial r}} (y-\ell_i)}} \label{eqn:contin1}\\
  &+& \E{\R \sim \state(t_i)}{\E{y \sim
      \adversM(t_i,\R)}{\at{\frac{\partial^2}{\partial r^2}}
      ((y-\ell_i)^2 - \deltat_i)}}
  \label{eqn:contin2}\\
  &+& O(s^3) \label{eqn:contin3}
\end{eqnarray}
Some care is needed here. we need to show that the expected value
are all finite. We assume that the expected potential
(Eq~(\ref{eqn:contin0}) is finite. Using
Lemma~\ref{lemma:infiniteexpectations} this implies that the expected
value of higher derivatives of $\frac{\partial}{\partial \R} \pot(\R)$
are also finite.\footnote{I need to clean this up and find an argument
  that the expected value for mixed derivatives is also finite.}


To prove inequality~(\ref{proof:onestep}), we need to show that the
terms~\ref{eqn:contin1} and \ref{eqn:contin2} are smaller or equal to
zero.
~\\~\\~\\
{\bf Term~(\ref{eqn:contin1}) is equal to zero:}\\
As $\ell_i$ is a constant
relative to $\R$ and $y$, and $\at{\frac{\partial}{\partial r}}$ is a
constant with respect to $y$ we can rewrite~(\ref{eqn:contin1}) as
\begin{equation} \label{eqnterm1.1}
  \E{\R \sim \state(t_i)}{\at{\frac{\partial}{\partial r}}
    \E{y \sim \adversM(t_i,\R)}{y} }
- \ell_i \E{\R \sim \state(t_i)}{\at{\frac{\partial}{\partial r}}}
\end{equation}

Combining the definitions of $\ell(t)$~(\ref{eqn:ell-discrete}) and~
and the learner's strategy
$\learnerM^{cc}$~(\ref{eqn:learner-strat-cc}) we get that
\begin{eqnarray}
\ell(t_i) &=& \E{\R \sim \state{t_i}}{\frac{1}{Z}
              \at{\frac{\partial}{\partial r}}
              \E{y \sim \adversM(i,\R)}{y}} \mbox{ where }
              Z=\E{\R \sim \state{t_i}}{\frac{1}{Z}
              \at{\frac{\partial}{\partial r}}}
              \label{eqnterm1.2}
\end{eqnarray}

Plugging~(\ref{eqnterm1.2}) into (\ref{eqnterm1.1}) and recalling the
requirement that $\ell(t_i)<\infty$ we find that
term~(\ref{eqn:contin1}) is equal to zero.


~\\~\\~\\
{\bf Term~(\ref{eqn:contin2}) is equal to zero:}\\
As $\deltat_i$ is a constant relative to $y$, we can take it
outside the expectation and plug in the definition of $\deltat_i$ (\ref{eqn:deltat})
\begin{equation} \label{eqn:term2.1}
  \E{\R \sim \state(t_i)}{\E{y \sim
      \adversM(t_i,\R)}{\adversM(t_i,\R)}{\at{\frac{\partial^2}{\partial r^2}}
      (y-\ell(t_i))^2} - \deltat_i}=
  \deltat_i - \deltat_i =0
\end{equation}
Where $G(t_i,\R)$ is defined in Equation~(\ref{eqn:SecondOrderDerivative})
We find that (\ref{eqn:contin2}) is zero.

Finally (\ref{eqn:contin3}) is negligible relative to the other terms
as $s \to 0$.
\qed 


\end{document}

%% deleted stuff


The next Lemma is the main part of the proof of
Theorem~(\ref{thm:IntegerGameBounds}). We use the backward induction
from Theorem~(\ref{thm:backward-recursion}) To compute upper and lower
potentials (Equations~(\ref{eqn:upperPotentials},\ref{eqn:lowerPotentials})) for
Strategies~(\ref{eqn:adv-strat-p}) and~(\ref{eqn:learner-strat-1})

The last iteration of the game: $i=T$ is the first step of the
backward induction. The upper and lower bounds are both set equal to
the first step in the backward induction we define
$$  \lowerpotb(T,\R) = \upperpotb(T,\R) = \pot(T,\R) $$

\iffalse
\begin{lemma} \label{lemma:first-order-bound}
  If $\pot(i,\R) \in \SP{2}$
  \begin{enumerate}
    \item The adversarial strategy~(Eq~(\ref{eqn:adv-strat-p}))
    guarantees the lower potential
 \begin{equation} \label{eqn:backward-iteration-lower}
   \lowerpotb(i, \R) = \frac{\lowerpotb(i,\R+1) + \lowerpotb(i,\R-1)}{2}
 \end{equation}
   
    \item The learner strategy~(Eq~(\ref{eqn:learner-strat-1}))
      guarantees the upper potential 
      \begin{equation} \label{eqn:backward-iteration-upper-recursion}
        \upperpotb(i, \R) = \frac{\upperpotb(i,\R+2) + \upperpotb(i,\R-2)}{2}
      \end{equation}
    \end{enumerate}
\end{lemma}
\fi


  \begin{eqnarray}
  \upperpotd(t_i,\R)&=&\E{\R \sim \state(i)}{\E{y \sim \adversM(t_i,\R)}{\upperpotd(t_i,\R+y-\ell(t_i))}}\\
  &\leq & \E{\R \sim \state(t_i)}{\frac{\upperpotd(i,\R+s_k(1+s_k))+\upperpotd(i,\R-s_k(1+s_k))}{2}}\\
  &+&
      \E{\R \sim \state(i)}{\E{y \sim \adversM(i)(\R)}{(y-\ell(i))
      \frac{\upperpotd(i,\R+s_k+s_k^2)+\upperpotd(i,\R-s_k-s_k^2)}{2}}}
  \end{eqnarray}


(Eq~\ref{eqn:learner-strat-1}). We follow the same line of argument as the second part of the proof of
Lemma~\ref{lemma:first-order-bound} to give a recursion for the upper
potential. The critical difference between the integer game is and the
discrete game is that in the discrete game $\ell(t_i)\leq s_i^2$ which
implies that $(y-\ell(t_i)) \in [-s_k(1+s_k),s_k(1+s_k)]$. This yields 


  Following the same line of argument as the first part of the proof of
Lemma~\ref{lemma:first-order-bound} we consider the time points:
$t_i=i s_k^2=i 2^{-2k}\realT$ for $i=0,1,\ldots,2^{2k}$.

  \begin{equation} \label{eqn:lower-discrete}
    \lowerpotd(t_{i-1},\R) = \frac{\lowerpotd(t_i,\R-s_k)+\lowerpotd(t_i,\R+s_k)}{2}
  \end{equation}


  %%%%%

  
Follow the proof of
  Lemma~\ref{lemma:first-order-bound}.Derive upper and lower scores
  for the two strategies. Show that they converge to the same thing as
  $k \to \infty$.




We start with the high-level idea. Consider iteration $i$ of the
continuous time game. We know that the adversary prefers $s_i$ to be
as small as possible. On the other hand, the adversary has to choose
some $s_i>0$. This means that the adversary always plays
sub-optimally. Based on $s_i$ the learner makes a choice and the
adversary makes a choice. As a result the current state $\state(t_{i})$
is transformed to $\state(t_i)$. To choose it's strategy, the learner
needs to assign value possible states $\state(t_i)$. How can she do
that? By assuming that in the future the adversary will play
optimally, i.e. setting $s_i$ arbitrarily small. While the adversary
cannot be optimal, it can get arbitrarily close to optimal, which is
Brownian motion.

Note that the learner chooses a distribution {\em after} the adversary
set the value of $s_i$. The discrete time version of $\learnerM^1$

In the discrete time game the adversary has an additional choice, the
choice of $s_i$. Thus the adversary's strategy includes that choice.
There are two constraints on this choice: $s_i \geq 0$ and
$\sum_{i=1}^n s_i^2 = T$. Note that even that by setting $s_i$
arbitrarily small, the adversary can make the number of steps - $n$ -
arbitrarily large. We will therefor not identify a single adversarial
strategy but instead consider the supremum over an infinite sequence
of strategies.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:

\iffalse
  We study two particular potential functions which achieve good simultanous regret bounds.

\subsection{Min/Max solution}

Theorem~\label{thm:simulBoundAveragePot} shows a one-to-one relationship between a simultanous regret bound
$B(\R)$ and the average potential function $\phi(\R)=B(\R)^{-1}$. We therefor ask what is the smallest possible $B(\R)$. Recall that the brownian motion adversary forces a cumulative loss of zero on any learner. The SBR for Brownian motion is the tail of the normal distribution:
\newcommand{\erf}{\mbox{erf}}
\begin{equation}
  B^*(\R) = \frac{1}{\sqrt{2 \pi}}  \int_{\R}^\infty e^{-\frac{x^2}{2\realT}}dx
  = \frac{1}{2} \paren{1-\erf\paren{\frac{R}{\sqrt{2\realT}}}}
\end{equation}
The final potential function that correspond to
\begin{equation}
  \finalPotR^*(\R) = (B^*(\R))^{-1} = \frac{2}{1-\erf\paren{\frac{\R}{\sqrt{2\realT}}}}
\end{equation}
One can check that $\finalPotR^* \in \SP{4}$ and therefor Brownian motion is the optimal strategy and
$$\pot^*(t,\R)= \E{\rho \sim {\cal N}(\R,\realT-t)}{\frac{2}{1-\erf\paren{\frac{\rho}{\sqrt{2\realT}}}}}$$
The bound on the final potential is:
\[
  \pot^*(0,0) =  \E{\rho \sim {\cal N}(0,\realT)}{\frac{2}{1-\erf\paren{\frac{\rho}{\sqrt{2\realT}}}}}
\]

\fi

\subsection{Anytime Potentials} \label{sec:stable}



\iffalse
\begin{itemize}
\item We provide min-max analysis for potential and quantile based online
  algorithms with a finite known horizon and a potential function with
  four strictly positive derivativs. In particular, we show that
  Brownian motion is the optimal adversarial strategy for any such
  potential function.
\item We show how to remove the finite horizon assumption and derive
  both exponential weights and NormalHedge potentials.
\item We provide second order bounds on the parameter-free normal-Hedge algorithm
  that depend only on the percentile $\epsilon$ and a variant of the cumulative
  variance. We show that this bound is the optimal bound for any
  potential based algorithm.
\item We construct a lower bound that holds for any online algorithm
  and is very close to the normal hedge upper bound.
\end{itemize}

We start with the bounded horizon case. Fixing a potential function at
the end of the game $\pot(T,\R)$ and the strategies used by the
learner and the adversary, we define potential functions $\pot(i,\R)$
for iterations $i = T-1,T-2,\ldots,0$ such that the score $\score(t)$ is guaranteed to 
be equal for all of the iterations.
\[
  \score(T) = \score(T-1)=\cdots=\score(0)
\]
This allows us to analyze the game one iteration at a time and
construct good strategies for both sides. We name this potential based
game the {\em Integer Time Game}, the analysis of this game is given
in Section~\ref{sec:int-time-game}. The analysis assumes only that the
final potential $\pot(T,\R)$ is strictly positive and has strictly positive
first and second derivatives (We denote the set
of functions that have $0,\ldots,k$ strictly positive derivatives by $\SP{k}$,
the formal definition is given in Section~\ref{sec:preliminaries})

The strategies yielded by the analysis guarantee bounds on the final
score. The adversarial strategy guarantees
$\lowerscore{} \leq \score(T)$, while the learner's strategy
guarantees $\score(T) \leq \upperscore{}$. Unfortunately, these bounds
don't match, i.e. $\lowerscore{}<\upperscore{}$. In other words our
proposed strategies are not min-max optimal. The question of whether
there exist min/max strategies for the integer time game is open.

To find min/max strategies we expand the game. We call the expanded
game the {\em discrete time game}. The expansion involves giving the
more options to the adversary, but not to the learner. As a result,
any upper bound $\upperscore{}$ that holds for a learner strategy in
the discrete time game also holds in the integer time game.

The added option for the adversary is to declare, at the beginning of
each iteration, the range of values of the instantanous losses. In the
integer time game this range is set to $[-1,+1]$. In the discrete time
game the range is chosen by the adversary on iteration $i$ to be
$[-s_i,+s_i]$ for $1\geq s_i >0$. To keep the game balanced between
the adversary and the learner we replace the iteration number $i$ with
real valued {\em time} parameter and let $t_{i+1}=t_i+s_i^2$. this and
another necessary adjustment are described in
Section~\ref{sec:discrete}. Section~\ref{sec:disc-game-strategies}
describes strategies used for the discrete game which are scaled
versions of the strategies for the integer time game.

We fix the potential at the end of the game $\finalPotR \in \SP{4}$
and consider a sequence of adversarial and learner strategies indexed
by $k$: $\adversMdk,\learnerMdk$, where
$\forall i,\; s^k_i = \frac{\sqrt{\realT}}{2^k}$ for some constant
$\realT$. We prove two facts regarding the limit $k \to \infty$.  The
first (Thm.~\ref{thm:seq-of-adv-strategies}) is that
$\lim_{k \to \infty}\upperpotMdk - \lowerpotMdk \to 0$.  The second
(Thm.~\ref{thm:smallerSteps}) is that, if $\pot(T,\R)$
$$\forall k, \forall 0\leq i \leq 2^{2k},t_i=i 2^{-2k} \realT,\; \forall\R,\;\;\;
,\lowerpotMdkpar{k+1}\left(t_i,R\right) >
\lowerpotMdk\left(t_i,R\right)$$ Taken together these facts imply
that, if the fixed potential function for the end of the game
$\finalPotR$ is in $\SP{4}$, then there exists a potential function
$\pot(t,\R)$. The adversarial strategy corresponding to this potential
function corresponds to Brownian motion.  The backwards recursion used
to computer the potential for $t \leq \realT$ is a partial
differential equation known as the Kolmogorov Backward equation.

The main result of this paper is that a {\em single} adversarial
strategy, i.e. Brownian motion, is optimal for any
sufficiently convex potential functions.

The discrete time game presents the adversary with a dilemma.  On the
one hand, The adversary has to declare, on each iteration, an upper bound on the
range of the losses $[-s_i,s_i]$ where $s_i>0$. On the other hand, it
wants to set $s_i$ as small as possible.~\footnote{The situation is
  similar to a folklore game in which each player writes down a number
  on a piece of paper and the player with the largest number wins.}

We introduce a variant of the game called the {\em continuous time
  game} to alleviate this dilemma. In this came the adversary does not
announce the step size and the learner behaves as if
the step size is infinitesimally small. In this case time is advanced according
to the variance of the actual losses. This much more natural algorithm yields
a regret bound that depends on the cumulative variance and is smaller
for easy, low variance sequences.

Until this point our theory holds for any final potential function in $\SP{4}$. We conclude by analyzing two specific potentials.
\begin{enumerate}
\item We derive a potential function and a corresponding learning algorithm
  that is min/max optimal for a given time horizon $\realT$.
  The optimality is in the sense that the
  simultanous regret bound for time $\realT$ has a matching
  simultanous lower bound.
\item By finding solutions to the Kolmogorov Backward equation that
  hold for all $t>0$ we eliminate the need to define a final
  potential. As a result we get an ``anytime'' learning algorithm that
  can be stopped at any time. The specific potential we analyze is
  Normal-Hedge~\cite{}. NormalHedge is not min/max optimal for any
  time, but it is almost optimal for all times.
\end{enumerate}




\section{Main Results}

\fi

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:


\section{Upper and lower bounds on the simultanous regret}
Plan: (1) first order bound with Variable $\nu$ to show optimality.
(2) Second order bound with $\nu=1$

We now combine Theorems \ref{thm:simulBoundAveragePot} and~\ref{thm:variancebound} with
the Normal-Hedge potential~(\ref{eqn:NormalHedge}) to derive a second order bound on the regret of NormalHedge.
\begin{equation}
  \R_{\mbox{\tiny NH$(\nu)$}}(\epsilon) \leq \sqrt{(t_i+\nu) \left(2 \ln \frac{1}{2\epsilon}+
      \ln (t_i+\nu)\right)}
\end{equation}
Where $t_i = \nu+\sum_{j=1}^{i}\deltat_j$
\begin{equation}
  \deltat_i=
  \E{\R \sim \state(i)}{H(t_i,\R) \;\; \E{y \sim \adversM(t_i,\R)}{(y-\ell_i)^2}}
\end{equation}
and 
\begin{eqnarray}
  W(t_i,\R)&=& e^{\frac{R^2}{2t_i}} \left(\frac{1}{t^{3/2}} + \frac{R^2}{t^{5/2}} \right)\\
  H(t_i,\R)&=& \frac{W(t_i,R)}{\E{\rho \sim \state(i)}{W(t_i,\rho)}}
\end{eqnarray}

The initial potential is $1/\sqrt{\nu}$ and it remains this way in the continuous case. In the discrete case it is $1/\nu + O(1/n)$ where $n$ is the number of steps We can upper bound the potential by
$1/\nu+1$

\iffalse
\section{The concept of state and  a critique of quadratic variation}
Central to our analysis is the concept of the {\em state} of the DTOL
game described in figure~\ref{fig:DTOL}. A state is a function of
history that is minimally sufficient to compute the future of the
game.  Clearly, a sufficient state of the game at time $t$ is the
vector of $N$ cumulative regrets: $(R_1^t,\ldots,R_N^t)$. As the
indexing of the actions is immaterial we can replace the vector with
a distribution of regrets at time $t$ which we denote $\state(t)$.

This definition of state is sufficient in two senses. First, the
regret bounds at time $t$ depend only on $\state(t)$. Second, given
$\state(t)$ and the actions of the learner and the adversary at time
$t$ uniquely defines $\state(t)$. Other information about individual
actions, such as the quadratic variation of individual actions, do not
effect the progression of the states.

To sharpen this argument consider an example depicted in
Figure~\ref{fig:counterExample}~\footnote{This example appeared in an
  open problem in COLT2016} We compare two loss histories, each
involving two actions. The first consists of the the blue and the red
lines, the second consists of the blue and the green line. In both
cases the total loss of both action at the end of phase 3 are
equal. In other words, the state at the end of phase 3 is identical
for the two examples. An algorithm that depends on quadratic variation
would give different weights to the high and the low variation action
even though the states are identical.~\footnote{One can argue for a
  different game in which the state includes the quadratic
  variation. However, we can't see a natural Way to define a game that
  requires the expanded state.}
\begin{figure*}[th] \label{fig:counterExample}
\includegraphics[width=\columnwidth,height=3in]{OpenProblemFigure.pdf}
\end{figure*}
\fi
\section{Integer time game}
\label{sec:int-time-game}
The integer time game is described in
Figure~\ref{fig:integerTimeGame}.  The integer time game generalizes
the decision theoretic online learning problem~\cite{FreundSc97} in
the following ways:
\begin{enumerate}
\item The goal of the learner in DTOL is to guarantee an upper bounds
  on the regret. The learner's goal in the integer time game is to
  minimize the final score. From theorem~\ref{thm:simulBoundAveragePot} we know that if
  we set the final potential as $\finalPotT(\R) = \frac{1}{G(\R)}$ then the two
  conditions are equivalent, allowing us to focus on the score.
\item The number of iterations $T$ is given as input, as is the
  potential function at the end: $\finalPotT(\R)$.
\item The actions are assumed to be {\em divisible}. For our purposes
it is enough to assume that any action can be split into two equal
weight parts.
\end{enumerate}

The key to the potential based analysis is that using the predefined
final potential we can define potential functions and scores for all
iterations $1,\ldots,T-1$. This is explained in the next subsection.

The final score is calculated: $\score(T)=\state(T) \odot
\finalPotT$.

The goal of the learner is to minimize this score, the goal
of the adversary is to maximize it.
\end{minipage}}
\caption{The integer time game \label{fig:integerTimeGame}}
\end{figure}

\subsection{Defining potential Functions for all iterations\label{sec:potentials}}

The potential game defines the {\em final} potential function
$\finalPotT$, at the end of the game. We will now show, that we can
extend the definition of a potential function to all iterations of the game.

 A single action defines a path $S_\omega$ (as defined in (\ref{eqn:path})). Fixing
 the strategies of the learner and the adversary determines
 a distribution $\D$ over paths.
We describe two equivalent ways to define $\potPQ(i,\R)$ for $i<T$
 \begin{enumerate}
 \item{\bf Using conditional expectation} We can define the potential
   on iteration $i$ based on the fixed potential at iteration $T$.
% Using the definition of prefix sum given
% in Equation~(\ref{eqn:prefix-sum}, we can write this conditional
% expectation as 
\begin{equation}
  \forall i=1,\ldots,T,\; \forall \R \;\;\;
  \potPQ(i,\R)=\E{\omega\sim \D|\R_\omega^i=\R}{\pot(T,\R_{\omega}^T)}
\end{equation}
 \item{\bf Using backward induction} It is sometimes convenient to
   compute the the potential for time $i$ from the potential at time
   $i+1$:
   \begin{equation} \label{eqn:back-induction}
     \forall i=1,\ldots,T-1, \R\;\;\;
     \potPQ(i,\R)=\E{\omega\sim \D|M_\omega^i=\R}{\potPQ(i+1,\R_{\omega}^{i+1})}
   \end{equation}
by using backwards induction: $i=T-1,T-2,\ldots,1$ we can compute the
potential for all iterations.


We use Equations~(\ref{eqn:Bias},\ref{eqn:aggregate-loss}) and
marginalizing over $\R$ to express Equation~(\ref{eqn:back-induction})
in terms of the single step strategies:
\begin{equation} \label{eqn:induction}
  \forall i=1,\ldots,T-1, \R\;\;\;
  \potPQ(i,\R) \doteq \E{r \sim [(\R-\ell(i)) \oplus \adversM(i,\R)]}{\potPQ(i+1,r)}
\end{equation}

\end{enumerate}

The score at iteration $i$ is defined as
$\score(i)=\state(i)\odot \pot(i)$. The scores are all different
expressions for calculating the expected final potential for the fixed strategies
$\adversM, \learnerM$. Therefor the scores are all equal, as expressed
in the following theorem:

\begin{theorem} \label{thm:backward-recursion}
Assuming $ \learnerM(i,\R), \adversM(i,\R)$ are fixed for all
$i=1,\ldots,T-1$, then
\[
  \state(T)\odot \pot(T)=\score(T)=\score(T-1)=\cdots=\score(1)=\potPQ(0,0)
  \]
\end{theorem}

A few things worth noting:
\begin{enumerate}
\item $\potPQ(i,\R)$ is the the final expected potential
  given that the paths starts at $(i,\R)$ and that
  the strategies used by both players in iterations $i,\ldots,T$ are fixed. Note
  also that which strategies were used in iterations $1,\ldots,i-1$ is
  of no consequence. The effect of past choices is captured by the
  state $\state(i)$.
\item
  The final expected potential is equal to $\pot(0,0)$ which is the
  potential at the common starting point: $i=1$, $\R=0$.
\end{enumerate}

\subsection{Strategies for the integer time  game} \label{sec:strat-integer}
We assume that $\finalPotT  \in \SP{2}$, in other words, the final
potential is positive, increasing and convex. $\finalPotT$ defines the
upper and lower potentials at time $T$:
$$\lowerpotb(T,\R) = \upperpotb(T,\R) = \finalPotT(\R) $$
We define a backwards recursion for the lower potential:
\begin{equation} \label{eqn:backward-iteration-lower}
  \lowerpotb(i-1, \R) = \frac{\lowerpotb(i,\R+1) + \lowerpotb(i,\R-1)}{2}
\end{equation}
and a backwards recursion for the upper potential:
\begin{equation} \label{eqn:backward-iteration-upper-recursion}
  \upperpotb(i-1, \R) = \frac{\upperpotb(i,\R+2) + \upperpotb(i,\R-2)}{2}
\end{equation}

We define strategies that correspond to these potentials. A strategy
for the adversary:
\begin{equation} \label{eqn:adv-strat-p}
  \adversMb(i,\R) =
  \begin{cases}
    +1 & \mbox{ w.p. } \frac{1}{2}\\
    -1 & \mbox{ w.p. } \frac{1}{2}\\
  \end{cases}
\end{equation}
and a strategy for the learner:
\begin{equation} \label{eqn:learner-strat-1}
\learnerMb(i,\R) = \frac{1}{Z} \frac{\pot(i,\R+2) - \pot(i,\R-2)}{2}
\end{equation}
Where $Z$ is a normalization factor
$$Z = \E{\R \sim \state(i)}{\frac{\pot(i,\R+2) - \pot(i,\R-2)}{2}}$$

\begin{theorem} \label{thm:IntegerGameBounds}
  Let $\finalPotT \in \SP{2}$, for any iteration $0 \leq i \leq T$ and
  regret $\R_0 \in \reals$ 
  \begin{itemize}
  \item
    The lower potential guaranteed by $\adversMb$ is
     $$\lowerpotb(i,R_0) = \E{\R \sim \R_0 \oplus \Binom(T-i,1)}{\finalPotT(\R)} $$
  \item
    The upper potential guaranteed by $\learnerMb$ is
    $$\upperpotb(i,R_0) = \E{\R \sim \R_0 \oplus \Binom(T-i,2)}{\finalPotT(\R)}$$
  \end{itemize}
\end{theorem}

Plugging in $i=0$,$\R=0$ we get the following Corrolary:
\begin{corollary}
  if the learner plays $\learnerMb$ on every iteration
  it guarantees that the final score satisfies
  $$\state(T) \odot \finalPotT \leq \Binom(T,2) \odot \finalPotT $$

  If the Adversary plays $\adversMb$ on every iteration it guarantees
  that:
  $$\state(T) \odot \finalPotT = \Binom(T,1) \odot \finalPotT $$
\end{corollary}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
